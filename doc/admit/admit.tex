%% ADMIT.TEX
%% 11-aug-2013 astute -> admit

\documentclass[preprint]{aastex} % AASTeXv5.0
\usepackage{carma_memo}

\begin{document}
\carmamemo{999} 	

\title{ADMIT: ALMA Data MIning Toolkit}

\author{Peter Teuben}
\affil{University of Maryland}

\begin{abstract}

An overview of ADMIT (ALMA Data Mining Toolkit) V0.5 is given. This is
the outcome of an ALMA Development Study awarded to Mundy and Varshney
(2012), in preparation for an ALMA Development Project, due August 16,
2013. Cut and paste at your hearts content.

This memo describes the technical aspects of ADMIT, some working, and
some still a mockup. All working and example codes are available in a
CVS module named {\tt astute}, including this memo.

% ==== a bit about the what it is
ADMIT operates on ALMA data. Within the ALMA pipeline environment
it will first construct ``data added value'' components that enables
an archive user to then quickly acess the scientific value of the data,
before downloading the actual data. ADMIT will then assist in
recomputing selected results, and optionally finetune them for the user.


ADMIT consists of two main components. The core is the infrastructure layer
that defines data structures and programmatic pipeline; provides methods
for I/O, data selection, and computation of scientifically relevant
quantities; extracts and organizes scientific metadata from the cubes; and
defines the application programming interface (API) for higher-level tools.
Built upon the infrastructure layer are the ADMIT CASA Tools that provide
advanced functionality for scientific analysis.

%===

ADMIT is targeted at both a novice user (via a convenient GUI), as
well as an experienced user (Python toolkit within the CASA
framework), and is designed to be extensible via plugins.

\bigskip
{\bf CAVEAT}: ADMIT was formerly known as ASTUTE, and you will still
find the latter name where the former is meant.


\end{abstract}


\ChangeRecordBegin
\addrevision{0.1}{2013-Aug-11}{P. Teuben}{}
{Relevant material transferred from image-io memo and adopted ASTUTE to ADMIT}
\addrevision{0.2}{2013-Aug-16}{P. Teuben}{}
{material for ALMA Development Proposal}
\ChangeRecordEnd


\section{Introduction}

ADMIT (ALMA Data MIning Toolkit)\footnote{formerly known as ASTUTE. 
This document will still use the name ASTUTE in places where ADMIT is meant.}
is a toolkit that operates on ALMA data, and from existing data cubes, 
creates a number of ``added value'' datasets and descriptions 
that summarize to the user
what science they can expect from these data. Some of these data
are then wrapped into what
we call the {\tt admit.zip} file, and contain both the descriptive XML
file, and all associated -- but small -- FITS and JPG files.

The ALMA pipeline ingests visibility data (in ASDM format) and after calibration
and mapping produces a series of image datacubes, one for each band (currently 
NB=4 for ALMA, with NC=3840 channels per band), and one for each distinct
source (usually NS=1, but depending on the science 
goals the project has established, multiple sources are possible). If
single dish is available, and/or a source contains multiple fields,
the ALMA pipeline will combine them into one single source 
mosaiced image datacube.

ADMIT will then prepare a small summary for the archive, enabling users
to get an overview before the actual large data has been downloaded.

ADMIT contains two components: the infrastructure and the plugins.


\section{ADMIT Tools outline}

\label{s:tools}

Here we outline a number of ADMIT Tools (``plugins'').  Although they
are presented in a serial fashion, there is some advantage of a
waterfall type model, where for example you redo the statistics based
on the knowledge where the signal is, which was derived from later steps. At
each step new meta-data is gathered that describes the data.

At each stage ADMIT will return data structures in Python, for easy
integration in casapy or any other Python module accessible by
CASA. Some examples of this are given in code fragments in the 
Section~\ref{s:scenarios}.

First we will size up the arrays and tables used in ADMIT, with the 
following dimensions:

\begin{enumerate}

\item {\bf NP:}
Number of projects. In the ALMA pipeline this is of course one, but ADMIT
can also supervise and datamine many projects, in which case this number
is relevant and larger than 1.

\item {\bf NS:}
Number of sources per project. This is usually 1, but it can be more.

\item {\bf NB:}
Number of independant correlator bands, per project/source.

\item {\bf NC:}
Number of channels in a band.

\item {\bf NL:}
Number of lines detected accross all bands.

\item {\bf NV:}
Number of velocity channels in a line cube

\item {\bf NY:}
Number of Y (DEC) pixels in a cube

\item {\bf NX:}
Number of X (RA) pixels in a cube

\item {\bf NF:}
Number of features detected in a line cube

\end{enumerate}

In the data structure array notation we will use below, the C-convention is used, 
meaning the first indices are the ones on top of the hierarchy, i.e. slowest moving.
For example $cube[NP][NS][NV][NY][NX]$ would be a set of $(NX,NY,NV)$ cubes, 
for $NP$ projects, but in each project one can have a different number ($NS$) of
sources.

Now we will describe a few of the modules in the ADMIT pipeline.


\subsection{Data Summary}

This creates a container in which the number of projects (NP),
sources (NS), bands (NB), lines (NL) and features (NF) will be set. 
This, and 
associated tables describing these, will determine the workflow in the
ADMIT pipeline. In each subseqent step this container (maintained in XML) will
be updated such that at activating ADMIT in a project, it will quickly
present a summary of everything that has been gathered and computed. 

One of the first components of this data summary is a simple cube
statistics table with properties such as min, max, mean, 
robust median, rms per channel,
which can be the input for the next step of line identification.

$$
cube[NP][NS][NB][NC][NY][NX] \rightarrow stats[NP][NS][NB][NC]
$$
where 
$$
     cube = \{ intensity [, mask] \}
$$
and
$$
stats = \{min,max,rms,mean,median,...\}
$$

A {\it (robust) median} is useful, but makes the procedure more expensive.


\subsection{Line Identification}

A {\it line-strength} vs. {\it frequency} table is the essential input
into a line identification procedure (see e.g. datasplat).
The input can be as simple as the previously computed cube statistics, 
but aided by cross-correlation techniques in a PV diagram, or the cube itself,
aided by where the emission is (see also Figure~\ref{fig:pvcorr4}
and Section~\ref{s:line}).
The output will be a series of identified and unidentified lines accross the different
bands, NL in total, therefore reducing a dimension of NB*NC to NL.
This is a simple table of 
{\it line, frequency, detection probability}, with NL entries.

$$
    line[NP][NS][NL]
$$
with
$$
    line = \{name\_id, frequency, probability\}
$$


\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{pvcorr4.eps}
\caption{Line identification can be tremendously improved in a PV diagram
by cross-correlating the signal in the V direction. Compare the more traditional
noisy peak/rms plot in the middle (in white) 
with the smooth cross-correlation technique on
the right (in red). The two vertical lines denote zero and a 3 sigma clipping 
above which we deem the line significant.}
\label{fig:pvcorr4}
\end{figure}


\subsection{Line Cubes}

Based on the line identification  line cubes can be extracted,
with now the third axis in doppler velocity space, to ensure
that we can compare the different lines and molecules on a channel
by channel basis.

$$
     linecube[NP][NS][NL][NV][NY][NX]
$$
where 
$$
     linecube = \{ intensity [, mask] \}
$$

Binning is an option, if signal to noise needs to be increased without
loss of spectral resolution, but the exact gridding needs to be 
the same.
Depending on the number of lines in the bands,
keeping only the line cubes can significantly cut down disk usage.
We denote the number of velocity channels by NV. 

For some interesting point in the moment 0 map (the peak?), a spectral profile
is stored by ADMIT to guide the user to the new 3rd dimension.

\subsection{Moment Maps}

In the previous step NL line cubes have been cut from the NB band cubes,
with the option to re-bin the cubes.
Cubes will now be labeled in doppler velocity space, with the intent
that all cubes have the same gridding applied.  Simple moment 0, 1 and 2 maps
are created with a selected method for where the emission is. This can 
be based on a simple RMS cutoff, but also
more sophisticated methods that include local smoothing to
increase the signal to noise to create a more robust mask that is then used
for the creation of the moment maps. Moment maps are exported to {\tt admit.zip}
for visual inspection.

$$
    map[NP][NS][NL][NY][NX]
$$
with
$$
    map = \{mom0, mom1, mom2, mask\}
$$

Moment maps can also be produced using parametric shapes, e.g. a gaussian fit, which
would then store the total emission, mean velocity and width of the spectrum at each
location instead of the classic moments.

\subsection{Feature Extraction}

Many sophisticated feature (``source'') extraction routines are available from the community
(clumpfind, dendrograms, gaussclumps, a more complete is given in Section~\ref{s:feature}),
which essentially produce
a table of properties for each feature. This table is stored as a VO Table 
in {\tt admit.zip}, with NF entries for each  detected feature. Features
are allowed to be hierarchical in nature, such as for dendrograms. 
The essential data structure is:

$$
   feature[NP][NS][NL]
$$
with
$$
   feature = \{X, Y, V, I, shape\}
$$
where {\it shape} is a more general description containing (amongst) the size
and orientation, and possibly higher order moments, at a given location
{\it X,Y,V} in a cube with integrated emission {\it I}. 


\subsection{Feature Characterization}

Since for each of the NL line cubes, a catalog of features is available,
characterization of the emission accross different molecules/lines is now 
possible on a scale never been seen before. Principle Component
Analysis (PCA) and an Overlap Integral (OI) are two ways to quantify
and visualize this. A linked data tool (e.g. {\tt glue}) can be 
of tremendous help here.

\subsection{Overlap Integral}

One way to get an overview of the type of emission accross all lines
is the overlap integral.
For each detected line one assigns a bit in an integer, and then logically ORs
them accross all lines, giving a map of a bit-mask of the emission present
at this location. One can do this in a map, or even in a cube, and study
in which regions certain lines are present and/or absent in relation
to other lines.

\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{n253_overlap8.eps}
\caption{Overlap integral: on the right the moment 0 map of one particular line,
on the left the combined overlap integral - valued 0..255 - of 8 lines, each
using a single bit in an 8 bit integer. As can be seen, most of the peaks
have all 8 lines present. There are some recombination lines present in this
cube, which clearly have a different spatial distribution.}
\label{fig:overlap}
\end{figure}



\subsection{Principal Component Analysis}

An extension of the overlap integral would be to not use a simple on/off bitmask,
but create a vector containing the full strength for each line, and using
a Principle Component Analysis find out the basis vectors of what describes
the emission, including how many different regions there are, and what they
represent.

See Meier \& Turner (2005), where they took eight molecules 
observed with OVRO of IC 342.

\subsection{Saliency}

Saliency in a dataset is based on segmenting the emission and assigning
a description vector to the features found. Description vectors can be
picked from a number of properties of the emission. In vision research
(e.g. Ip et al, 2013)
a popular one is a color histogram, which enables a very fast
cataloging of the types of features, as well as finding the ones similar
to a selected one.


In astronomical data cubes, finding the best description vector is key
to finding the interesting features.  Extending the experience in
vision research, where a 3-channel RGB histogram was used, we can see
our NL lines as a ``color'' and assign a description vector based on 
this to efficiently catalog the types of regions that exist in large
datacubes.
Another example is a description
vector based on the moments of inertia, or those based pon a moment
0,1,2 map.  For a selected object, one can then ask what other objects
have similar description vectors, as in a PCA, and perform a cluster
analysis to decipher how many kinds of objects there are given the
description that was selected.



\section{Scenarios}

\label{s:scenarios}

Several scenarios are given how ADMIT can be used.  For nomenclature, we continue
to use {\bf P} for
the projects, {\bf S} for the sources, {\bf B} for the bands (spectral windows), 
{\bf L} for the identified and unidentified lines, and {\bf F} for the extracted
features in a line cubes. They count to 
NP projects, NS sources, NB bands, NL lines and NF features.
These spectral window bands are independent sections on the frequency axis, 
and can contain both line and continuum radiation. NL will then refer to all the
lines in all the NB bands.
The continuum radiation is allowed to have a spectral 
index (or in worst case: a true parameterized shape, e.g. black body).


In each of the 4 code scenarios we will expose the enourmous power of the
ADMIT toolkit in handling such a new situation.

\subsection{Pipeline: initialize a single project}

This would be the mode in which ADMIT runs at the tail end of the ALMA pipeline
in the archive, i.e. from one or more spectral
windows (ALMA: NP=1, NS=1, NB=4, NL$\ge$0)\footnote{do we need to say something about 
polarization here? Not on 2 yr timescale?} 
the previously described steps produce ancillary data for the
{\it admit.zip} file.

\footnotesize
\begin{verbatim}
# code0.py:

import astute

as = astute.Astute()               # initialize from the current directory
as.pipeline(0)                     # run a default pipeline
as.save()                          # save everyting in admit.zip

\end{verbatim}
\normalsize


\subsection{User: query archive for a single project}

This example is the case where a user has selected one project (NP=1)
in which one source was present (NS=1), but 4 associated 
spectral windows (NB=4, NC=3840). In these cubes 19 spectral lines 
were detected (NL=19),
resulting in 19 line data-cubes, with associated moment-0,1,2 maps in FITS
and JPG format. An additional feature identification was made, available
in tabular format.  The user visualizes them, and extracts the 19
data cubes, and recomputes moment maps with a slightly less conservative
setting of the clipping, and recomputes feature detection and compares
them with the ones provided previously by ADMIT.

\footnotesize
\begin{verbatim}
# code1.py:


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

projects = ar.query('gal && line(CO) && z<0.2 && T>1')
np = size(projects)

for p in projects:
    as.setdir(p.name)                 # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:                 # project 'p' and a series of 'c' cubes
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsection{User: recompute properties using different algorithms}

In this case the user has decided to select an undefined line, identify it, 
and add this to the ADMIT database, in addition, suggest a feedback to the ALMA data archive
(speculative) for this particular project.
Recomputing and re-correlating data is an essential ingredient of ADMIT.

\footnotesize
\begin{verbatim}
# code2.py:


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

#  search here and below for astute.xml files
#  optionally a query for science, e.g. only CO lines
projects = as.query_dir('.','line(CO)')
np = size(projects)

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsection{User: query archive for many projects and cross-correlate clump properties}

In this example the user has harvested a series of CO cubes via
an archive query that resulted in many object/projects (NP=121).
From these CO cubes, clumps have been identified using a modified
algorithm from the one the ADMIT pipeline had used, and the slope
of the mass spectrum is correlated against the mass of the galaxy.
This really shows some of the more advanced power in ADMIT
of gathering datasets (be it via an archive query, or assembly by
hand or other means), and datamining these dataset with ADMIT tools.
Under the hood these tools are not limited to CASA tools and tasks, but
can also be taken from external packages with minimal work (cf.
ParselTongue).

\footnotesize
\begin{verbatim}
# code3.py:

import astute

as = astute.Astute()

projects = as.query_dir('.','line(CO)')
np = size(projects)

#  we want to plot L vs. S
s=[]
l=[]

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper project directory
    s.append(p.getpar("s"))
    l.append(p.getpar("l"))
    #
#


\end{verbatim}
\normalsize

\subsection{User: A multi-source investigation}

In this use case scenario the user has already downloaded a number
of galaxies from the ALMA archive, and made sure they include a CO
line cube and are ADMIT enabled.
The CO distribution is clumpy and resolved, but is
generally in a plane where the clumps are rotating around the center
of the galaxy. The hypothesis is that the clump mass spectrum slope is
dependent on the total mass of the galaxy, which in turn depends on
the measured rotation speed at some maximum measured distance. In this
particular case the user decides to recompute the clumps using a ``clumpfind''
algorithm with a more conservative setting of the noise. The resulting
spectrum is computed and a slope fitted.  For the mass, a simple
rotation curve is fitted, and the mass is derived from the
outermost radius and rotation speed.  A scatter plot is created of
the slope vs. log(M).

Here is some sample ADMIT code how you could do this:

\newpage

\footnotesize
\begin{verbatim}
import admit, numpy, scipy.stats, math     # grab some needed modules

adm = admit.ADMIT()                        # initialize ADMIT

projects = adm.query_dir('.','line(CO)')   # look for CO cubes
np = size(projects)                        # see how many we got


m_list = []                # accumulate masses of the galaxies
a_list = []                # accumulate slopes of the clump mass spectra

for p in projects:
  adm.setdir(p.dirname)    # move into the proper project directory
  for c in p.cubes:        # loop over cubes (should be only 1 !!!)

    rms = 3 * p.rms                        # set a new rms for clumpfind
    f  = adm.feature(c,'clumpfind',dt=rms) # recompute clump list
    cm = f.getcol('mass')
    logm = numpy.log10(cm)
    (x,y) = numpy.histogram(logm)
    slope = scipy.stats.linregress(x,y[1:])[0]
    a_list.append(slope)                   # Mass Spectrum Slope

    m1 = c.moment(1)                           # reference to the moment-1 map
    rc = adm.rotcur(m1)                        # fit a rotation curve 
    rad = rc.getcol('r')                       # grab array of radii
    vel = rc.getcol('vrot')                    # grab array of rotation speeds
    rmax = rad.max()                           # radii, in kpc
    vmax = vel.max()                           # in km/s, inclination corrected
    dist = rc.getpar('distance')               # distance, in mpc
    m_list.append(math.log10(vmax*vmax*rmax))  # log(Galaxy_Mass), ignore G
  #
#
m = numpy.array(ml)
a = numpy.array(al)
adm.plot2d(m,a)                            # show on a plot


\end{verbatim}
\normalsize

\newpage

\subsection{User: A multi-source investigation}


In this use case scenario the user has already downloaded a number of
projects from the ALMA archive, and all have the basic ADMIT data
included. The hypothesis to be checked is if the linewidth in CO
correlates with the presence of a selected number of molecular lines,
in the sense that for wider profiles some of the molecules would be
destroyed, and thus the overlap integral would not achieve the maximum
value (where all molecules are present).  In this case the user
selected $CO$ as the reference line, and $C^{17}O$, $CS$ and
$H^{13}CN$ as probes. In this example the user is more interested in a
global value per object, than a point by point comparison in the map.


The code loops over all projects, and first ensures that all lines are
present. It estimates the line width from the PeakProfile (precomputed
by ADMIT) in the $CO$ linecube as a representative value for the
linewidth for the object.  This particular user is conservative and
wants to recompute the Moment-0 maps with a 3-sigma clip. These new
Moment-0 maps are then turned into an overlap map (one of the ADMIT
tools) and via some simple NumPy math the fraction is computed from
number of pixels with value 15 (where all lines are present) divided
by the number of pixels with any signal in any of the lines (overlap
map value more than 0).  The line width is then plotted against this
fraction, hoping that is starts near 1 and then suddenly drops at some
linewidth.

Here is some sample ADMIT code how you could do this:

\newpage


\footnotesize
\begin{verbatim}  
import admit, math                             # grab some needed python modules
import numpy as np                             #

adm = admit.ADMIT()                            # initialize ADMIT

projects = adm.query_dir('.')                  # look for ADMIT projects here
np = size(projects)                            # see how many we got

lines = ['co', 'c17o', 'cs', 'h13cn']          # first line is reference line
omax = math.pow(2,len(lines))-1                # max in overlap map, 15 in this case

s_list = []                                    # accumulate linewidths
f_list = []                                    # accumulate fractions

for p in projects:
  adm.setdir(p.dirname)                        # move into the proper project directory
  line_cubes = {}
  for c in p.linecubes:                        # loop over line cubes and grab the line name
    if lines.count(c.line):                    # if we got one of the lines we wanted
       line_cubes[c.line] = c                  # store reference to that linecube

  if len(lines) != len(line_cubes):
    print "Skipping ",p.dirname
    continue

  c_ref   = line_cubes[lines[0]]               # reference to the CO cube
  x       = c_ref.peakprofile('vlsr')          # spectrum X in KM/S
  y       = c_ref.peakprofile('value')         # spectrum Y in JY/BEAM
  x_mean  = (x*y).sum()/y.sum()
  x_width = (x*x*y).sum()/y.sum() - x_mean*x_mean 

  s_list.append(x_width)                       # accumulate for later

  m = []                                       # accumulate maps for new overlap
  for l in lines:                           
    m0 = adm.moment(c,[0],'clip',rms=3*c.rms)  # compute new moment maps
    m.append(m0)

  o = adm.overlap(p,m)                         # get overlap image
  oval = o.array()                             # get a numpy reference for work
  f_all = len(np.where(oval == omax)[0])       
  f_sig = len(np.where(over > 0)[0])

  f_list.append(f_all/f_sig)
  #
#
adm.plot2d(s_list,f_list)                      # scatterplot of accumulated data 


\end{verbatim}
\normalsize
\newpage

\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{mock1.eps}
\caption{Example GUI: On the left from top to bottom the Inspector, Moment, 
Morphological Analysis, Principal Component Analysis, and Spectral Cutout
actions for the data selected in the middle panel. On the top row tab's are
visible for a number of collected sources. NOTE: the middle panel will be replaced
with real figures from ADMIT/matplotlib}
\label{fig:gui1}
\end{figure}



\section{ADMIT Pipeline}


ADMIT consists of two components: an infrastructure to run
and maintain a pipeline, and a series of ADMIT tools. The pipeline
infra-structure is loosely based on the MIS pipeline (Pound \& Teuben, 2011?),
where the tools are simply CASA tools working within the ADMIT environment.


\section{Archive Access}

Some of ADMIT's functionality depends on how well it hooks into the archive,
and until that is open, ADMIT will depend on manually downloading the cubes
for (re)processing.  By default, ADMIT can only show a graphic summary of
a dataset, and will need large cubes

Is it going to depend on VO services? The SIAV2 data access protocol
will handle multi dimensional image data cubes, an early draft is expected
by Aug 16, and a SIAV2 prototype / reference implementation is planned to 
be demoed at the IVOA interop during ADASS this year (Tody, priv.comm.)

Or is it going to depend on a direct API into the archive?  


\subsection{admit.zip}

We wrap our XML data (tables etc.) and any associated files (all meant to be small)
into the {\tt admit.zip} file.
Python has good methods to manage zip files, maintenance will be done in 
a temporary (?) subdirectory per project. Code example in azip.py

There are two types of {\tt admit.zip} files: the one per project, that describes
all data and science products, and one that describes projects.

Possible hierarchy we find in {\tt admit.zip}: (products with * are large and don't come
with ADMIT and have to be recomputed and refetched from archive)

\subsection{ADMIT SDP}

The ADMIT Basic Data Products (BDP) are are gathered in a single tar
file: an XML describing the project and associated images and tables
is one of those files. Although this is not cast in stone, we do have
a rough outline of the contents of the XML file, which you can see
reflected in a number of descriptions throughout this proposal.

\footnotesize
\begin{verbatim}
ProjectName
    Nsource
    SourceName1
       Ra
       Dec
       Vlsr
       Nband
       Band1
           BandCube (*)
           FreqSetup
           CubeStats
       Band2
       ...
       Nline
       VelSetup
       Line1
           LineCube (*)
           PeakProfile
           Mom0
           Mom1
           Mom2
           FeatureList
       Line2
       ...
       OverlapIntegral
       ...
    Sourcename2
    ...    
\end{verbatim}
\normalsize


\section{Related Technologies}

There are a few notes on existing technologies that 
impact further development of ADMIT\footnote{where missing, the wiki probably has 
links and more info}:

\begin{enumerate}

\item
CASA is using Qt (not GTK+) as their GUI. Our mockup version with glade is GTK+ based,
so we'll have to switch as soon as we go and do the real thing.

\item
The now working VLA pipeline (Chandler) 
is written in casapy, and would be worthwhile studying in more detail.

\item
The {\tt ParselTongue} package (ascl:1208.020, Kettenis et al, 2006ASPC..351..497K)
is a python package that lets you run AIPS (and other) tasks. 
They have many methods that resemble what ADMIT has to do. In particular, having
an option to wrap other common packages (e.g. MIRIAD) in ADMIT, and/or have an option
to add any package (AIPS, OBIT, NEMO, ....) would be a powerful addition
that allows flexible python scripting at a high level.

\item
Related to the previous two item: lets not forget our own CADRE (ascl:1303.017, Friedel 2013)

\item
CASA's own {\tt casaviewer} is a client/server program which should be at least as good
as {\tt ds9} to interactively view image cubes. ADMIT should make use of that, but also
give an option to use {\tt ds9} if a user already has this loaded and wants to compare/blink  
from another package. Plans exist to replace {\tt casaviewer}.

\item
The {\tt datasplat} interface to Splatalogue (Kent \& Remijan) is 
a good example that we can expand on/ collaborate
on adding line identifications to ADMIT, although we need a number of optional preprocessors
to extract a good {\it line-strength} vs. {\it frequency} table that is the input 
to {\tt datasplat}. One of the strengths of ADMIT is that this extraction method is
flexibly set or determined via heuristics.

\item
The linked data concept from the Python-based 
{\tt glue} project is highly suited to build tasks in our environment.

\item
The visualization tools in the python-based {\tt yt} project are impressive, although
these are based on theory. Bolatto's N253 CO data cube received much attention in the
literature with Rosalowski assisted in producing a 3D visualization of the clumps
in N253.

\item
{\tt ipython} has a {\it notebook} capability, much like Mathematica's Notebook,
which allows one to play back (think verifyability). In another way one could see
this as parallel to the {\tt admit.zip} file. notebooks are stored in JSON
format.


\end{enumerate}


\section{Line Identification}
\label{s:line}

The topic of detection and identification of spectal
lines in a complex source datacube is a complex one,
of which we offer several solutions. 

\begin{enumerate}

\item
Measuring a robust RMS in a channel (the intent being that the source is removed from the 
RMS calculation), and comparing this to the peak value in a channel gives a good 
indication of a line detection. For channels that do not contain any signal, 
the PEAK/RMS will typically be around 3 or 4, depending on the size of the map.
Note that for wide spectral windows, the RMS will vary as function of frequency,
as well as be biased where strong lines are present (or large areas of the map
contain sources). The RMS then needs to be interpolated. In NEMO the code
{\tt ccdstat planes=0} will compute plane based statistics.

\item
Inspecting a typical position-velocity (PV) cut through a cube, makes it apparent
that lines can be detected easily by eye, as their shapes are often related
(this is technically not always true, e.g. recombination lines and molecular lines have
obviously a different origin and work under different ISM conditions). Nonetheless,
by using a well known (and identified) line as a template, it can be moved up and down
along the V axis in a PV diagram, and a cross correlation then results in another method
to identify lines. The experimental codes are in NEMO as {\tt pvcorr}.


\item
A related and derived case of this
cross-correlation technique would be to compute the intensity
weighted velocity within the template, and move this curved line up and down in V and resample
the PV diagram along this shifted line, then add up all the emission along this curved
sample, and this will also be an indication of the detected lines.  Obviously this
method will generate more noise, but will typically have $\sqrt{2}$ more resolution.

Also consider hyperfine structures, coadding their features to increase signal-to-noise.

\item
To balance the noise, a certain amount of smoothing to the data will always help
the identification.


\end{enumerate}

Special care has to be given to the calibration of the lines. Given the nature of
the template and the not well defined velocity of the source, the template line
must have an assumed identity and known rest frequency  ($f_{1R}$). 
With a known Doppler velocity $V_{lsr}$ its sky frequency ($f_1$) 
can then be computed.  Assuming the unknown line is measured ${\Delta f}$ away, one
can show that the rest frequency of the unknown line is given by:


$$
 f_{2R} = f_{1R} + {  {\Delta f} \over { (1-z) }}
$$

\section{Feature Extraction}
\label{s:feature}

Separating signal from noise is one of the most important first steps
in the analysis of an image or a data cube. Many algorithms have
been developed and published over the last 30 years. They basically
can be classified in two main  categories:  thresholding and pattern
matching.

Some recent reviews are available:
Popping et al. 2012,  Hassan (2012) Thesis Section 5.   
% koribalski 2012 - targeted source finding techniques
% Koribalski, B., 2012. Overview on spectral line source finding and visualisati
on. 
% Publications of the Astronomical Society of Australia. xix, 10, 131, 140, 141,
 226
% http://adsabs.harvard.edu/abs/2012PASA...29..359K


Here we review some methods in common use with published papers. 
We are ignoring a very similar cottage industry in the N-body (points)
world. A good recent review and code comparison 
by Knebe et al. (2013) can be found
in {\tt http://arxiv.org/abs/1304.0585}.

ASCL references listed below can be found off {\tt http://ascl.net}, as well
as in ADS {\tt http://adsabs.harvard.edu/}. This is probably the easiest
reference point to find everything related to the listed software.

\subsection{clumpfind}

The Williams et al. 1994 
ClumpFind
\footnote{http://adsabs.harvard.edu/abs/1994ApJ...428..693W}
algorithm has been implemented as {\tt clfind}
in miriad, but is also available in an IDL version. 
{\bf ascl:1107.014}
Now superseded
by {\tt cprops} (CloudProps, Rosolowski and Leroy 2006) {\bf ascl:1102.012}
\footnote{http://arxiv.org/abs/astro-ph/0601706/}.

\subsection{Dendogram}

Also of note is the dendogram approach,



\subsection{duchamp / selavy}

Whiting (2011,2012) for 2D and 3D. Used in ASKAP pipeline. 
{\tt selavy} is the ASKAP 21-cm version. {\bf ascl:1201.011}
\footnote{http://adsabs.harvard.edu/abs/2012MNRAS.421.3242W}

\subsection{blobcat}

Hales et al. 2012.  2D.
{\bf ascl:1208.009}

\subsection{getsources}

Menshchikov et al. 2012. Herschell based. Two dimensional.
\footnote{http://adsabs.harvard.edu/abs/2012A\&A...542A..81M}
Code is not freely available yet, but is expected to be. Example
output made available to us.

\subsection{gaussclumps}

The original stutzki code is also available in GILDAS, as well as in
STARLINK.  It iteratively decomposes a 3D cube into a series of
Gaussian shaped clumps.  It starts by fitting a 3-dimensional clump
locally to the maximum of the input lbv cube. It then subtracts this
clump from the cube, creating a residual map, and then continues with
the maximum of this residual map.  The procedure is repeated until a
stop criterion is met, for instance when the maximum of the residual
maps drops below the 3 sigma level.

{\tt gaussclumps} was developed by Stutzki \& Guesten (1990, ApJ, 356, 513)
who applied it to their M17SW data sets. It was later described and
analyzed in more detail by Kramer et al. (1998, A\&A, 329, 249) who
also applied it to data cubes of various Galactic clouds.  The papers
of e.g. Heithausen et al. (1998) and Simon et al. (2000) give more examples
of the application of gaussclumps to large scale CO maps.

\subsection{imsad}

{\tt imsad} = Image Search and Destroy. 
Implemented in MIRIAD, based on {\tt imfit}, esssentially fits a gaussian
to a peak that sticks out of some plateau.

\subsection{SExtractor}

A widely source extractor (Bertin \& Arnouts 1996), but not so good for blended 
objects.
{\bf ascl:1010.064}

\subsection{SFind}

Hopkins et al. 2002.   Implemented in MIRIAD.

\subsection{MultiFind}
perl script using a series of miriad commands. Kilborn 2001. Written for HIPASS.

\subsection{tophat}

Meyer et al. 2004. For HIPASS. Uses a pattern matching of shapes and cross corre
lation
to find signals.

\subsection{lovedata}

Saintonge 2007. For ALFALFA. Written in IDL. Pattern matching.


\subsection{GammaFinder}

Boyce 2003. Masters thesis Cardiff.  In java. Use gamma function to estimate
noise and reject non-signal

\subsection{1d-1d wavelet}

C++ by FLoer and Winkel 2012.

\subsection{smooth plus clip}

An old method from the WSRT Bosma time, but more formally
introduced by Serra et al. 2011. ATLAS3D.

\subsection{CNHI}

Jurek 2012. for WALLABY project design study. CNHI = 
Characterised Noise HI Source Finder. Pattern matching. Also mentions the
Lutz (1980) one path algorithm, which is used to group regions from
the same source into a single source.


\subsection{DisPerSE}

DisPerSE is open source software for the identification of persistent
topological features such as peaks, voids, walls and in particular
filamentary structures within noisy sampled distributions in 2D,
3D. Sousbie (2013) {\bf ascl:1302.015}
\footnote{http://www2.iap.fr/users/sousbie/web/html/indexd41d.html}
 

Using DisPerSE, structure identification can be achieved through
the computation of the discrete Morse-Smale complex. The software can
deal directly with noisy datasets via the concept of persistence (a
measure of the robustness of topological features). Although developed
for the study of the properties of filamentary structures in the
cosmic web of galaxy distribution over large scales in the Universe,
the present version is quite versatile and should be useful for any
application where a robust structure identification is required, such
as for segmentation or for studying the topology of sampled functions
(for example, computing persistent Betti numbers). Currently, it can
be applied can work indifferently on many kinds of cell complex (such
as structured and unstructured grids, 2D manifolds embedded within a
3D space, discrete point samples using delaunay tesselation, and
Healpix tesselations of the sphere). The only constraint is that the
distribution must be defined over a manifold, possibly with
boundaries.

\subsection{FIVe}

Hacar et al. (2013) discuss a new Friends of Friends (FOF) 
algorithm they dubbed {\tt FIVe} (Friends in Velocity), and
implemented in the R scripting language.
\footnote{http://arxiv.org/abs/1303.2118}


\subsection{Kemper}

A multi-scale approach to automated feature extraction from HI
data cubes. 
So far only published as a research note, Kempker (2005) from RUG.


\subsection{PyFind}

% Paolo Serra <serra@astron.nl>
Serra (2012) at WSRT/ASTRON
has developed this not-yet-named code. PyFind is his current 
placeholder name. 

It is a simple Python code working on FITS files. It smooths
the cube using a set of specified kernels and finds emission at each
resolution by applying a specified threshold. The program applies also
a size filter at each resolution and makes a final mask as the union
of all masks. Two papers in 2012, one on MNRAS and one on PASA, discuss
it.

\subsection{GRID\_core}

Gong and Ostriker (2013),
coming from a more theoretical angle and 
instead of working with densities, developed 
{\tt GRID\_core} that works with
the gravitational potential.
In essense this is simply a more smooth version of
the density. It uses a watershed model (much like ClumpFind)
to define clumpy structures.  If the potential is not directly
available, a poisson solver can be be used to define densities
and with an assumed conversion factor, regions of bound structures
can be defined as well.  {\bf ascl:1302.007}


\subsection{TODO}

Here's some catch phrases that need to be tracked down if they have
relevant code for the source and feature extraction module.

GAIA, MATADOR, AstroMed, SPLAT-VO, Herschel DP, S2Plot, AstroMD. 


\section{Timeline and Deliverables}

In this section we describe the technical approach, the
timeline and the deliverables. Management Plan? 
All in a self-contained package.
2 years, 2.5 FTE is what we have on the whiteboard.

\subsection{Risk Factors}

\begin{enumerate}
\item
Pipeline Archive Integration (Low) : build on local fake archive?
\item
Pipeline Access API (Medium): user will manually have to download files
\item
Some tools too hard (Low): Deliver fewer tools

\end{enumerate}

\section { MARC: Overview }
The ALMA Data Mining Toolkit (ADMIT) is a software suite that operates on
ALMA data cubes. It creates descriptions that summarize to the user key
observational components of the data, as well as a number of "added value"
datasets to facilitate standard scientific analysis and to enable new and
creative ways to derive science from the cubes.  ADMIT is envisioned to
be run as part of the ALMA pipeline to create a new set of products for
the ALMA archive; however it is designed to also be capable of running
as a standalone application operating on user-downloaded cubes, including
recomputation (with perhaps modified parameters) of the products created
in the archive.

ADMIT consists of two main components. The core is the infrastructure layer
that defines data structures and programmatic pipeline; provides methods
for I/O, data selection, and computation of scientifically relevant
quantities; extracts and organizes scientific metadata from the cubes; and
defines the application programming interface (API) for higher-level tools.
Built upon the infrastructure layer are the ADMIT CASA Tools that provide
advanced functionality for scientific analysis.

\subsection{ Infrastructure Layer \label{s:infrastructure} }

% Using present tense here, but perhaps future tense is more appropriate.

ADMIT is fully defined and implemented in a CASA-compatible Python module.
The basic architecture is a pipeline with certain default image operations,
to which the user can add further operations.  The default pipeline
consists of the Summary, Moment, and Line Identification tools (see section
\ref{s:tools}).  ADMIT can work with images in both FITS and CASA format.
The outputs of the pipeline are wrapped into a single, self-describing
ZIP archive file, which any ADMIT CASA Tool can parse and manipulate.
A typical ZIP archive file might contain a descriptive XML file and 
associated small FITS and JPG files.

ADMIT is runnable from within the CASA Python environment, as individual
commands or a script.  However, for novice users we provide a simple
graphical user interface (GUI) for launching Tools and viewing results.

The core output of ADMIT is an XML file that contains all metadata
extracted from the science data, e.g.  project code, source name, sky
coordinates, ALMA bands used, list of spectral lines covered.  Each ADMIT
operation builds additional detail into the XML file creating a more
comprehensive, scientifically useful description.

In addition, ADMIT provides an infrastructure to operate on multiple
projects or sources, and perform the same operations on each of those. This
can be done either in parallel or recursively. It can then extract
information from each project and in essence mine a large suite of data,
allowing linked data techniques to visualize the extracted information
and provide new insight on what is common or different in the sources.
%% Are we providing a linked data viewer? Scope creep?


\subsection{ CASA Tools }

ADMIT defines a standard architecture that allows users to create and
"plug in" their own analysis tools to the ADMIT module.  ADMIT provides
an API for these tools to call existing CASA commands, manipulate images
or metadata, compute new quantities, return data structures to casapy,
and store results in the ZIP file with new metadata written to the XML file.

We describe here the N tools we will deliver at the end of the grant
period, as well as possible future tools.

{\it  PETER: for the moment I'm editing these in section \ref{s:tools}}

%a ZIP archive file that contains both the descriptive XML file, and all
%associated - but small - FITS and JPG files.


\section*{References}

\begin{enumerate}
\item
{\bf ASTUTE wiki:} {\tt  http://carma.astro.umd.edu/wiki/index.php/AStute}

\item
datasplot: URL

\item
Meier \& Turner, 2005. 2005ApJ...618..259M

\end{enumerate}


% ===============================================================================


\section{TAS (MIS) Pipeline}

We describe a adaptation of the MIS pipeline, called TAS, here. Because we
needed some simple quick hack on the XML based execution pipeline that was
not present and designed yet, and because MIS was available and understood
by our group, a quick one hour hack resulted in TAS, while the underlying
project was still called ASTUTE.

Some of the text below is taken from the MIS manual, which needs some adaptations.

We describe here a simple yet flexible and effective pipeliner for
Unix commands.  It uses a Makefile (behind the scenes) to define a
serial set of commands for your choice of the pipeline. The pipeline
commands share a common set of parameters by which they communicate.
Pipeline parameters can optionally be made persistent accross multiple
runs of the pipeline. Commands must follow a simple convention to
retrieve and store parameters.



\section{TAS Introduction}

To process a large number of datasets in a very similar 
way is a common theme that we have applied for a number of CARMA
observation. We thus developed
a simple infrastructure to assemble and run a pipeline comprising
of a set of commands that have to be run in a certain order,
and depend on each other. We call this 
package TAS, derived from MIS.

Technically {\bf tas} consists a set of shell and python scripts
and the infrastructure to setup a serial pipeline, and some aspects
of running this pipeline in parallel as well.

A summary of typical TAS pipeline commands is:

\footnotesize
\begin{verbatim}
  pipeline $MIS/cat/pipeline.001 > Pipefile               # generate control Pipefile
  pipepar -c project=c0184.3B_108PG2130.13 carmaRefant=2  # set some parameters
  pipe all                                                # run the pipe
\end{verbatim}
\normalsize   


\section{Pipeline}

Lets assume we have 4 programs (or scripts) called {\tt step1},
{\tt step2}, {\tt step3} and {\tt step4} that need to be run in
succession in each project directory. 
Each of the {\tt stepX} 
commands accepts its own unique set of {\tt keyword=value}
command line arguments, 
but the pipeline convention is that the values for each of these
keywords are remembered for any subsequent run if you do not
specify them explicitly. So if you would manually
first run a command as ``{\tt step2 foo=bar}'', 
the next time you would run just
``{\tt step2}'', it will have remembered the value
{\tt bar} for {\tt foo}. Users of the MIRIAD shell
program will recall this type of global parameter behavior. This
can be very convenient, but can also bite you when you least expect it.

With the ``{\tt pipeline}'' command you establish which commands,
and in which order, need to be executed for your pipe. This will
produce a Unix-style Makefile, which by convention we call a
{\tt Pipefile}:

\footnotesize
\begin{verbatim}
  % pipeline 4 step1 step2 step3 step4 > Pipefile
\end{verbatim}
\normalsize   

Of course these four commands\footnote{Although we gave
the command simple stepX names, you can name these any
way you like, e.g. {\tt pipeline 2 foo bar } is legal}
must exist in your Unix {\tt \$PATH} and
most follow the pipeline parameter convention (see below).

Unless for some bizarre reason you do not use pipeline
parameters, a dummy parameter file needs to be created
before you can run the pipeline:

\footnotesize
\begin{verbatim}
  % pipepar -c
\end{verbatim}
\normalsize   

Apart from manually running each command, the ``{\tt pipe}'' 
command will now run these 4 steps in succession:

\footnotesize
\begin{verbatim}
  % pipe
  Checking ./Pipefile
  Doing step1 as step1
  Doing step2 as step2
  Doing step3 as step3
  Doing step4 as step4
\end{verbatim}
\normalsize   

This command 
uses the Unix ``{\tt make}'' command, and uses the {\tt Pipefile}
as the controllng {\tt Makefile} with all dependancies properly defined.
If for some reason {\tt step2} fails, the pipe will be aborted at that stage. If
you try and re-run the pipe after it had been successfully
finished, you will probably see something like the following:

\footnotesize
\begin{verbatim}
  % pipe
  Checking ./Pipefile
  make: Nothing to be done for `all'.
\end{verbatim}
\normalsize   

Technically, it uses Unix dot files for its dependancies.
These dot files should not be touched or removed, unless you know what you are
doing. They make sure commands are not run again if not needed because
nothing was changed.

Here are some simple examples of running and re-running portions
of the pipeline

\footnotesize
\begin{verbatim}
pipe all               does all steps in the pipeline (if needed)
pipe step2             run just step2 (if even ran ok before)
pipe all               now will do step3 and step4
pipe step3 all         does step3 and step4
pipe clean             wipes the pipeline control files (the dot files)
\end{verbatim}
\normalsize

As was hinted to before, a utility is needed to manage the global
pipe parameter file. This is the file that contains the parameters
that are passed between the programs and scripts
in the pipeline. This file is a very simple ascii file with a
set of ``{\tt par=val}''
pairs, one per line. No spaces should be used before the parameter,
or surrounding the '=' sign. 
You can actually use any text editor to modify this file.
No quotes are needed as everything is interpreted as text.  
CHECK THIS FOR VALUES WITH SPACES

Here is the usage line for the {\tt pipepar} command
\footnotesize
\begin{verbatim}

pipepar [-1|2] [-h] [-c] [-l] [-f parfile_name] [-s shell] [-p fmt] [-d par] [-v par] [-e par] [-z par] [-a] [par=val ...]

 -h	    help
 -1         Use single quotes (default)
 -2         Use double quotes
 -c	    create empty tas.def (parfile_name)
 -l         use spaces when printing out (-v) comma separated keyword values
 -f file    use another parfile_name from the default tas.def (not recommended)
 -s shell   output for given shell (csh and bash are currently known)
 -p fmt     C-style format (e.g. %10.2f) used in printing (-v) values
 -v par	    show value of a parameter (multiple allowed, but do not combine with -e flag)
 -d par     delete a parameter
 -e par     print 1 or 0 if a parameter exists (only only occurence allowed now)
 -z par     sort a multi valued (comma separate) value
 -a         append a value (in a comma separated way) to the existing value(s)
 par=val    assign (new) value to a parameter  (multiple allowed, -a will append)  

\end{verbatim}
\normalsize



\section{Some Examples}

\begin{enumerate}

\item
rerun the complete pipeline (from step1 onwards) with a new parameter

\footnotesize
\begin{verbatim}
   pipepar foo=1.3
   pipe step1 all
\end{verbatim}
\normalsize

\item
rerun the pipeline with a new parameter, but only run the pipeline
from step3 onwards. This also implies you better make sure that {\tt foo}
is not needed in {\tt step1} and {\tt step2}. The current pipeline
has no dependancy for this yet.

\footnotesize
\begin{verbatim}

   pipepar foo=2.3
   pipe step3 all
\end{verbatim}
\normalsize


\item
Rerun piece of the pipeline in each of
a set of directories. In cumbersome {\tt csh} notation this 
could be achieved as follows:
\footnotesize
\begin{verbatim}

   foreach dir (`cat dirs.txt`)
     cd $dir
     pipepar foo=3.3
     pipe step3 all
     cd ..
   end
\end{verbatim}
\normalsize


and because this is somewhat cumbersome to type, a special command is 
available to help running (pipeline) commands in a set of directories:

\footnotesize
\begin{verbatim}
  piperun dirs.txt 'pipepar foo=3.3 ;  pipe step3 all'
\end{verbatim}
\normalsize

does the same thing

\item
From a set of directories, create a scatterplot of two pipeline
derived parameters. Again, with the aid of
the {\tt piperun} command this can be done as follows

\footnotesize
\begin{verbatim}
   piperun dirs.txt pipepar -v foo -v bar > foo_bar.tab
   tabplot foo_bar.tab
\end{verbatim}
\normalsize

\item
Run a (pipe) command on a set of project directories in parallel. Generally
you will need to understand if your pipeline contains I/O and if running
them in parallel is an efficient usage of your resources. And of course
the number of processes you can run them under. Here is an example
of running a set of pipelines on 4 processors, from step2 and onwards:

\footnotesize
\begin{verbatim}
   piperun -n 4 dirs.txt pipe step2 all
\end{verbatim}
\normalsize


\item
Re-create a new pipeline using an extra step, and rerun the whole pipeline

\footnotesize
\begin{verbatim}

   pipeline $MIS/cat/pipeline.002  > Pipefile   
   pipe clean all
\end{verbatim}
\normalsize
You generally will need to use an extra {\tt clean} step, in order
to clean up the old ``dot'' control files, since the steps are unlikely
to be compatible with the old pipeline (the only exception being if
the new pipeline appends steps to the old one).

\item
Run the MIS pipeline in parallel on a 4-way processor
on a set of project directories, and store the
output in pipe.log (in each project directory):

\footnotesize
\begin{verbatim}
  piperun -n 4 -c -o pipe.log dirs.txt 'pipepar -c project=%s showPlots=False; pipe clean all'
\end{verbatim}
\normalsize

Note the use of the special ``{\tt \%s}'' construct to replace it with the active
directory name from the {\tt dirs.txt} file as it loops over the project directories.
Here is the full usage line for the {\tt piperun} command:

\footnotesize
\begin{verbatim}
Usage: piperun [-n #procs] [-o logfile] [-c] [-v] [-w] dirs.txt cmd [args]
-h         this help
-n #procs  parallel processing using #procs processors
-o logfile output log
-c         create directories
-v         verbose (debug)
-w         wait for RETURN after each project
dirs.txt   text file with directory names
cmd        unix command to run
args       arguments to unix command (inlcuding ; cmd2 args2...)
\end{verbatim}
\normalsize

% piperun -c -o pipe.log dirs.txt 'pipepar -c project=%s showPlots=False; pipe clean all'
%
\item
Re-Run the TAS pipeline for failed projects

\footnotesize
\begin{verbatim}
  % piperun -v -o pipe1.log dirs.txt 'pipepar showPlots=False; pipe all'
\end{verbatim}
\normalsize

CAVEAT: There appears to be a Unix issue aborting this command with the usual \^C,
it will instead work on the next project directory. A better way to halt the
series, is to issue \^Z, which suspends the task, and then issue 
``{\tt kill \%\%}'' the currently last suspended task.

\item
Re-Run a project with some additional flagging :
\footnotesize
\begin{verbatim}
  % echo 'ant(2)'  > CARMA.uvflag
  % echo 'ant(21)' > SZA.uvflag
  % pipe reduction all
\end{verbatim}
\normalsize

Flagging is acted upon in the {\tt reduction} step, but
only if files {\tt CARMA.uvflag} and/or
{\tt SZA.uvflag} are present, they are created with an editor, or
in this case with a simple {\tt echo} command, and the pipeline
is run again.

If you want to store these uvflag files in a more
persisent way, use the advanced feature of
storing them as pipefiles (see ``Advanced Usage'' below) together
with {\tt tas.def} do the following

\footnotesize
\begin{verbatim}
  % pipepar pipefile=CARMA.uvflag,SZA.uvflag
  % pipesave 
\end{verbatim}
\normalsize

\end{enumerate}


\section{Pipeline Parameter Convention}

Any script or program that is part of the pipeline
must follow the same convention of retrieving named
parameters from the commandline and override any previously stored
values in the (default: {\tt tas.def}) pipeline parameter file.
At the (successfull) end of the script new and changed parameters
will be written back to this parameter file for the next program in
the pipe to pick up these variables.

In short, these are the conventions:

\begin{enumerate}
\item
arguments to a pipeline tasks should be a unique series of ``{\it par=val}''
(since parameters can be shared between pipeline tasks)
\item
values are stored as a string, assigning a type (integer, real, boolean etc.) are
left open to the task
\item
each task should define a default value for each parameter
\item
the task should then read the global database (usually from the {\tt tas.def} file)
\item
task can now compute
\item
task should write out all parameter back to the global database
\end{enumerate}

\subsection{csh}
Here is a very simple example in the {\tt csh} scripting language how
this can be accomplished:

\footnotesize
\begin{verbatim}
#! /bin/csh -f
#

#  (1) define default values in case not given
set a=1
set b=2

#  (2) pipeline interface to grab old defaults
pipepar -s csh > tmp$$.par;  source tmp$$.par;  rm tmp$$.par

#  (3) poor man's command line processor to override parameters
foreach _arg ($*)
  set $_arg
end

#  (4) The Actual Code where the work can be done
echo A=$a   B=$b

#  (5) write pipeline parameters back
pipepar a=$a b=$b c=3
\end{verbatim}
\normalsize   

The actual code is in (4), though (1) and (3) are fairly common techniques
to make your shell script more dynamic and have it accept parameters in
a simple way. (2) and (5) are the new pipeline based commands to ensure
previous defaults are read before the commandline sets them (2), and 
ensure their values are stored back into your local pipeline database (5).

\subsection{python}
Here is a very simple example in the {\tt python} scripting language how
this can be accomplished:

\footnotesize
\begin{verbatim}

#! /bin/env python
#
import parfile, sys

a=1
b=2

if __name__ == "__main__":
   p = parfile.ParFile('tas.def')
   p.argv(sys.argv)
   p.set('a',a)
   if p.has('b'): 
      b = p.get('b')
   else:
      p.set('b',b)
   p.set('sum',a+b)
   p.save()
\end{verbatim}
\normalsize   

You can find full examples of this convention for
bash, csh and python in the {\tt \$MIS/templates/pipeline}
directory. There are no examples in C or Fortran yet, but
are relatively easy to implement and add to your library.


\section{Advanced Usage}

Two wrapper scripts exist that help you managing your pipeline
parameters in a persistent way. Imagine your pipeline directory tree
runs completely from scratch, so you would like to save and restore
parameters between different versions of the pipeline. We simply store
the parameter files ({\tt tas.def}) in an agreed upon location,
currently the {\bf project} directory within the {\tt \$MIS\_DEF}
(if present, or else the default {\tt \$MIS/def}) directory), and
retrieve them in a subsequent run. The commands {\tt pipesetup} and
{\tt pipesave} are used for this
\footnotesize
\begin{verbatim}
  pipesetup a=1 b=2 project=test
  pipe all
  pipesave
\end{verbatim}
\normalsize   

Another common technique to store large amounts of simulations
is in a hierarchy where each level identifies a new value in a
parameter. {\bf mis} supports this method as well, by
storing the project directories in that same hierarchy. 
In the example below you see three levels representing values for the
3 parameters {\tt a, b} and {\tt c}:

\footnotesize
\begin{verbatim}
  mkdir par/0.4/0.1/6.0
  pushd par/0.4/0.1/6.0
  pipepar -c project=par/0.4/0.1/6.0 a=0.4 b=0.1 c=6.0
  pipe all
  pipesave
  popd 

...

  mkdir test
  cd test
  pipesetup project=par/0.4/0.1/6.0
  pipe all
  
\end{verbatim}
\normalsize   

In addition to the special identifying {\tt project} parameter, 
the {\tt pipefile} parameter is also treated somewhat special by the
{\tt pipesetup} and {\tt pipesave} wrappers: they can contain a comma
separated list of filenames that will be retrieved and saved at the start
and beginning of a pipeline, if you add these wrappers to your pipeline.
This would then enable your pipeline to use these files in a more persistent
way.

Example:
\footnotesize
\begin{verbatim}
  pipepar pipefile=NOTES,uvflags
  echo flagging ant 2 and 13, both had issues   > NOTES
  echo 'ant(2),time(10:00,11:00)'   > uvflags
  echo 'ant(13),time(11:00,12:00)' >> uvflags
  pipesave

\end{verbatim}
\normalsize 


In the case that you need multiple calls to set a series of {\tt pipefile}'s,
the {\tt -a} flag to {\tt pipepar} is needed, but to ease persistence, it 
is recommended you sort them using the {\tt -z} flag before saving the files,
viz.

\footnotesize
\begin{verbatim}
  pipepar -a pipefile=uvflag.mis
  ...
  pipepar -a pipefile=NOTES
  ...

  pipepar -z pipefile
  pipesave

\end{verbatim}
\normalsize 



\section{Building your own Pipeline}

\begin{enumerate}

\item
Define your pipeline commands, and for each command make sure they
follow the pipeline parameter convention. Place the commands somewhere
in your {\tt \$PATH}.
\item
Create the Pipefile, for example
\footnotesize
\begin{verbatim}
  pipeline 5 getdata calibrate map deconvolve summary > Pipefile
\end{verbatim}
\normalsize   
You can stick this Pipefile either in each project directory you
want to run the pipeline on, or make it a default by overwriting
the example in {\tt \$MIS/cat/Pipefile}
\item
Run the pipe, set parameters etc.etc.
\footnotesize
\begin{verbatim}
  pipepar -c foo=bar fum=bar
  pipe all
\end{verbatim}
\normalsize   


\item
If you want save/restore the pipeline parameters, the {\tt project}
parameter is the key for this:
\footnotesize
\begin{verbatim}
  pipepar project=mytest1
  pipesave
\end{verbatim}
\normalsize   

the next time you setup a pipeline in another directory, the command
\footnotesize
\begin{verbatim}
  pipesetup project=mytest1
\end{verbatim}
\normalsize   

will restore your previous defaults.


\end{enumerate}


\section{TAS Pipeline Command Summary} 

Although the current {\tt \$TAS/bin} directory also contains
many TAS specific commands, the following commands are very general
pipeline related, and all start with the 4 letters {\bf pipe}:

\footnotesize
\begin{verbatim}
  pipeline            create a Pipefile for running the 'pipe' command
  pipepar             set and retrieve pipeline parameters
  pipe                run the pipeline
  piperun             run (optionally in parallel) pipeline in set of directories

  pipesetup           grab previously run pipeline parameters for a project
  pipesave            save pipeline parameters for a project
\end{verbatim}
\normalsize   

\chapter{Installation}

Here we discuss the installation of TAS (ASTUTE) and 
comment on some related packages if you need them.

\section{MIS}

We use the same CVS repository as MIRIAD and NEMO, and the
installation is very similar. The module is called {\tt astute} and
once installed it will need CASA, MIRIAD, python + scipy/matplotlib and NEMO.

\footnotesize
\begin{verbatim}
  % cvs -d :pserver:anonymous@cvs.astro.umd.edu:/home/cvsroot co astute
  % cd astute
  % ./configure
  % source astute_start.csh
\end{verbatim}
\normalsize

From this point on you can put new scripts and python modules in
the appropriate places (e.g. {\tt \$TAS/bin} and {\tt \$TAS/lib}, 
or your own style somewhere 
in {\tt \$PATH}\footnote{because pipeline 
commands are executed in another directory,
scripts in the current directory will not work.}

\section{TAS parameters}

The global pipeline parameters are stored in ASCII format in
a small text file, by default this is called {\tt tas.def}. 
For our TAS pipeline the following
parameters listed in column 2 are
defined by the programs listed in column 1:

\footnotesize
\begin{verbatim}
pipesetup       project         <optional step for re-running pipes with old tas.def files>
                pipefile        optional comma separated list of to-be-saved filenames
                step            most pipeline scripts store their last successful step name here

getdata         rawdata         where tar.gz or miriad/mir files live
                project         e.g. cx323.1E_89NGC133.20


report          -               create form log files from listobs, uvlist, uvindex


pipesave        -               (any parameters in pipesetup also apply here)

\end{verbatim}
\normalsize


\section{MIS ISSUES}

BAD:

-once par has been set, hard to set another value

-one single pipeline....  but what if, as here, we have 2

-easy way to set a global def file to be read for each project?




\end{document}

