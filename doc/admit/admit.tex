%% IMAGE I/O
%% 17-oct-2011 First Draft     - Peter Teuben

\documentclass[preprint]{aastex} % AASTeXv5.0
\usepackage{carma_memo}

\begin{document}
\carmamemo{999} 	

\title{ADMIT: ALMA Data MIning Toolkit}

\author{Peter Teuben}
\affil{University of Maryland}

\begin{abstract}

An overview of ADMIT (ALMA Data Mining Toolkit) V0.5 is given. This is
the outcome of an ALMA Development Study awarded to Mundy and Varshney
(2012), in preparation for an ALMA Development Project, due August 16,
2013.

This memo describes the technical aspects of ADMIT, some working, and
some still a mockup. All working and example codes are available in a
CVS module named {\tt astute}, including this memo.

ADMIT is targeted at both a novice user (via a convenient GUI), as
well as an experienced user (Python toolkit within the CASA
framework), and is designed to be extensible via plugins.

{\bf CAVEAT}: ADMIT was formerly known as ASTUTE, and you will still
find the latter name where the former is meant.


\end{abstract}


\ChangeRecordBegin
\addrevision{0.1}{2013-Aug-11}{P. Teuben}{}
{Relevant material transferred from image-io memo and adopted ASTUTE to ADMIT}
\addrevision{0.2}{2013-Aug-16}{P. Teuben}{}
{material for ALMA Development Proposal}
\ChangeRecordEnd


\section{Introduction}

ADMIT (ALMA Data MIning Toolkit)\footnote{formerly known as ASTUTE. 
This document will still use the name ASTUTE in places where ADMIT is meant.}
is a toolkit that operates on ALMA data, and from existing data cubes, 
creates a number of ``added value'' datasets and descriptions 
that summarize to the user
what science they can expect from these data. Some of these data
are then wrapped into what
we call the {\tt admit.zip} file, and contain both the descriptive XML
file, and all associated -- but small -- FITS and JPG files.

The ALMA pipeline ingests visibility data (in ASDM format) and after calibration
and mapping produces a series of image datacubes, one for each band (currently 
NB=4 for ALMA, with NC=3840 channels per band), and one for each distinct
source (usually NS=1, but depending on the science 
goals the project has established, multiple sources are possible). If
single dish is available, and/or a source contains multiple fields,
the ALMA pipeline will combine them into one single source 
mosaiced image datacube.

ADMIT will then prepare a small summary for the archive, enabling users
to get an overview before the actual large data has been downloaded.

ADMIT contains two components: the infrastructure and the plugins.


\section{ADMIT Tools outline}

\label{s:tools}

Here we outline a number of ADMIT Tools (``plugins'').  Although they
are presented in a serial fashion, there is some advantage of a
waterfall type model, where for example you redo the statistics based
on the knowledge where the signal is, derived from later steps. At
each step new meta-data is gathered that describes the data, and makes
it accessible to recompute by a user after they have downloaded the
data.

At each stage ADMIT will return data structures in Python, for easy
integration in casapy or any other Python module accessible by
CASA. Some examples of this are given in code fragments in the next
Section.


\subsection{Data Summary}

This creates a container in which the number of projects (NP),
sources (NS), bands (NB), lines (NL) and features (NF) will be set. 
This, and 
associated tables describing these, will determine the workflow in the
ADMIT pipeline. In each subseqent step this container (maintained in XML) will
be updated such that at activating ADMIT in a project, it will quickly
present a summary of everything that has been gathered and computed. One of
the first components of this data summary is a 
simple cube statistics table with properties such as 
min, max, mean, rms per channel, which can be the input for the next step.

$$
cube[NP][NS][NB][NC][NY][NX] \rightarrow stats[NP][NS][NB][NC]
$$
where 
$$
     cube = \{ intensity [, mask] \}
$$
and
$$
stats = \{min,max,rms,mean,median,...\}
$$


\subsection{Line Identification}

A {\it line-strength} vs. {\it frequency} table is the essential outcome
that will be input into a line identification procedure (see e.g. datasplat).
The input can be as simple as the previously computed cube statistics, 
but aided by cross-correlation techniques in a PV diagram, or the cube itself,
aided by where the sources are (see also Section~\ref{s:line} and next two steps).
The output will be a series of identified and unidentified lines accross the different
bands, NL in total. This is a simple table of 
{\it line, frequency, detection probability}, with NL entries.

$$
    line[NP][NS][NL]
$$
with
$$
    line = \{name\_id, frequency, probability\}
$$

\subsection{Moment Maps}

From the previous step NL cubes will be cut from the NB bands, bringing the data
volume down. In addition, there is an option to re-bin the cubes in frequency
space. Cubes will now be labeled in doppler velocity space., with the intent
that all cubes have the same gridding applied.  Simple moment 0, 1 and 2 maps
are created with a selected method for source identification (but see also next
step). Moment maps (in FITS and/or JPG?) are exported to {\tt admit.zip}.

$$
    map[NP][NS][NL][NY][NX]
$$
with
$$
    map = \{mom0, mom1, mom2\}
$$



\subsection{Feature Extraction}

More sophisticated source extraction routines are available from the community
(clumpfind, dendrograms, gaussclumps, see list below in Section~\ref{s:feature}, 
which essentially produce
a table of properties for each feature. This table is stored as a VO Table 
in {\tt admit.zip}, with NF entries for each feature detected. Features
are allowed to be hierarchical

$$
   feature[NP][NS][NL]
$$
with
$$
   feature = \{X, Y, V, I, shape\}
$$
where {\it shape} is a more general description containing (amongst) the size
and orientation, and possibly higher order moments, at a given location
{\it X,Y,V} in a cube with total contained emission {\it I}.


\subsection{Feature Characterization}

Since for each of the NL line's, a catalog of features is available,
characterization of the emission accross different molecules/lines is now 
possible on a scale never been possible before. Principle Component
Analysis (PCA) and an Overlap Integral (OI) are two ways to quantify
and visualize this. A linked data tool (e.g. {\tt glue}) can be 
of tremendous help here.

\subsection{Overlap Integral}

For each detected line one assigns a bit in an integer, and then logically ORs
them accross all lines, giving a map of a bit-mask of the emission present
at this location. One can do this in a map, or even in a cube, and study

\subsection{Principal Component Analysis}


\subsection{Saliency}



\section{Scenarios}

Several scenarios are given how ADMIT can be used.  For nomenclature, we continue
to use {\bf P} for
the projects, {\bf S} for the sources, {\bf B} for the bands (spectral windows), 
{\bf L} for the identified and unidentified lines, and {\bf F} for the extracted
features in a line cubes. They count to 
NP projects, NS sources, NB bands, NL lines and NF features.
These spectral window bands can be independent sections on the frequency axis, 
and can contain both line and continuum radiation. NL will then refer to all the
lines in all the NB bands.
The continuum radiation is allowed to have a spectral 
index (or in worst case: a true parameterized shape, e.g. black body).


In each of the 4 code scenarios we will expose the enourmous power of the
ADMIT toolkit in handling such a new situation.

\subsection{Pipeline: initialize a single project}

This would be the mode in which ADMIT runs at the tail end of the ALMA pipeline
in the archive, i.e. from one or more spectral
windows (ALMA: NP=1, NS=1, NB=4, NL$\ge$0)\footnote{do we need to say something about 
polarization here? Not on 2 yr timescale?} 
the previously described steps produce ancillary data for the
{\it admit.zip} file.

\footnotesize
\begin{verbatim}
# code0.py:

import astute

as = astute.Astute()               # initialize from the current directory
as.pipeline(0)                     # run a default pipeline
as.save()                          # save everyting in admit.zip

\end{verbatim}
\normalsize


\subsection{User: query archive for a single project}

This example is the case where a user has selected one project (NP=1)
in which one source was present (NS=1), but 4 associated 
spectral windows (NB=4, NC=3840). In these cubes 19 spectral lines 
were detected (NL=19),
resulting in 19 data-cubes, with associated moment-0,1,2 maps in FITS
and JPG format. An additional feature identification was made, available
in tabular format.  The user visualizes them, and extracts the 19
data cubes, and recomputes moment maps with a slightly less conservative
setting of the clipping, and recomputes feature detection and compares
them with the ones provided previously by ADMIT.

\footnotesize
\begin{verbatim}
# code1.py:


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

projects = ar.query('gal && line(CO) && z<0.2 && T>1')
np = size(projects)

for p in projects:
    as.setdir(p.name)                 # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:                 # project 'p' and a series of 'c' cubes
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsection{User: recompute properties using different algorithms}

In this case the user has decided to select an undefined line, identify it, 
and add this to the ADMIT database, in addition, suggest a feedback to the ALMA data archive
(speculative) for this particular project.
Recomputing and re-correlating data is an essential ingredient of ADMIT.

\footnotesize
\begin{verbatim}
# code2.py:


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

#  search here and below for astute.xml files
#  optionally a query for science, e.g. only CO lines
projects = as.query_dir('.','line(CO)')
np = size(projects)

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsection{User: query archive for many projects and cross-correlate clump properties}

In this example the user has harvested a series of CO cubes via
an archive query that resulted in many object/projects (NP=121).
From these CO cubes, clumps have been identified using a modified
algorithm from the one the ADMIT pipeline had used, and the slope
of the mass spectrum is correlated against the mass of the galaxy.
This really shows some of the more advanced power in ADMIT
of gathering datasets (be it via an archive query, or assembly by
hand or other means), and datamining these dataset with ADMIT tools.
Under the hood these tools are not limited to CASA tools and tasks, but
can also be taken from external packages with minimal work.

\footnotesize
\begin{verbatim}
# code3.py:

import astute

as = astute.Astute()

projects = as.query_dir('.','line(CO)')
np = size(projects)

#  we want to plot L vs. S
s=[]
l=[]

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper project directory
    s.append(p.getpar("s"))
    l.append(p.getpar("l"))
    #
#


\end{verbatim}
\normalsize


\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{mock1.eps}
\caption{Example GUI: On the left from top to bottom the Inspector, Moment, 
Morphological Analysis, Principal Component Analysis, and Spectral Cutout
actions for the data selected in the middle panel. On the top row tab's are
visible for a number of collected sources. NOTE: the middle panel will be replaced
with real figures from ADMIT/matplotlib}
\label{fig:gui1}
\end{figure}



\section{ADMIT Pipeline}


ADMIT consists of two components: an infrastructure to run
and maintain a pipeline, and a series of ADMIT tools. The pipeline
infra-structure is loosely based on the MIS pipeline (Pound \& Teuben, 2011?),
where the tools are simply CASA tools working within the ADMIT environment.


\section{Archive Access}

Some of ADMIT's functionality depends on how well it hooks into the archive,
and until that is open, ADMIT will depend on manually downloading the cubes
for (re)processing.  By default, ADMIT can only show a graphic summary of
a dataset, and will need large cubes

Is it going to depend on VO services? The SIAV2 data access protocol
will handle multi dimensional image data cubes, an early draft is expected
by Aug 16, and a SIAV2 prototype / reference implementation is planned to 
be demoed at the IVOA interop during ADASS this year (Tody, priv.comm.)

Or is it going to depend on a direct API into the archive?  


\subsection{admit.zip}

We wrap our XML data (tables etc.) and any associated files (all meant to be small)
into the {\tt admit.zip} file.
Python has good methods to manage zip files, maintenance will be done in 
a temporary (?) subdirectory per project. Code example in azip.py

There are two types of {\tt admit.zip} files: the one per project, that describes
all data and science products, and one that describes projects.

Possible hierarchy we find in {\tt admit.zip}: (products with * are large and don't come
with ADMIT and have to be recomputed and refetched from archive)
\footnotesize
\begin{verbatim}
ProjectName
    Nsource
    SourceName1
       Ra
       Dec
       Vlsr
       Nband
       Band1
           BandCube (*)
           FreqSetup
           CubeStats
       Band2
       ...
       Nline
       Line1
           LineCube (*)
           VelSetup
           PeakProfile
           Mom0
           Mom1
           Mom2
           FeatureList
       Line2
       ...
    Sourcename2
    ...    
\end{verbatim}
\normalsize


\section{Related Technologies}

There are a few notes on existing technologies that 
impact further development of ADMIT\footnote{where missing, the wiki probably has 
links and more info}:

\begin{enumerate}

\item
CASA is using Qt (not GTK+) as their GUI. Our mockup version with glade is GTK+ based,
so we'll have to switch as soon as we go and do the real thing.

\item
The now working VLA pipeline (Chandler) 
is written in casapy, and would be worthwhile studying in more detail.

\item
The {\tt ParselTongue} package (ascl:1208.020, Kettenis et al, 2006ASPC..351..497K)
is a python package that lets you run AIPS (and other) tasks. 
They have many methods that resemble what ADMIT has to do. In particular, having
an option to wrap other common packages (e.g. MIRIAD) in ADMIT, and/or have an option
to add any package (AIPS, OBIT, NEMO, ....) would be a powerful addition
that allows flexible python scripting at a high level.

\item
Related to the previous two item: lets not forget our own CADRE (ascl:1303.017, Friedel 2013)

\item
CASA's own {\tt casaviewer} is a client/server program which should be at least as good
as {\tt ds9} to interactively view image cubes. ADMIT should make use of that, but also
give an option to use {\tt ds9} if a user already has this loaded and wants to compare/blink  
from another package. Plans exist to replace {\tt casaviewer}.

\item
The {\tt datasplat} interface to Splatalogue (Kent \& Remijan) is 
a good example that we can expand on/ collaborate
on adding line identifications to ADMIT, although we need a number of optional preprocessors
to extract a good {\it line-strength} vs. {\it frequency} table that is the input 
to {\tt datasplat}. One of the strengths of ADMIT is that this extraction method is
flexibly set or determined via heuristics.

\item
The linked data concept from the Python-based 
{\tt glue} project is highly suited to build tasks in our environment.

\item
The visualization tools in the python-based {\tt yt} project are impressive, although
these are based on theory. Bolatto's N253 CO data cube received much attention in the
literature with Rosalowski assisted in producing a 3D visualization of the clumps
in N253.

\item
{\tt ipython} has a {\it notebook} capability, much like Mathematica's Notebook,
which allows one to play back (think verifyability). In another way one could see
this as parallel to the {\tt admit.zip} file. notebooks are stored in JSON
format.


\end{enumerate}


\section{Line Identification}
\label{s:line}

The topic of detection and identification of spectal
lines in a complex source datacube is a complex one,
of which we offer several solutions. 

\begin{enumerate}

\item
Measuring a robust RMS in a channel (the intent being that the source is removed from the 
RMS calculation), and comparing this to the peak value in a channel gives a good 
indication of a line detection. For channels that do not contain any signal, 
the PEAK/RMS will typically be around 3 or 4, depending on the size of the map.
Note that for wide spectral windows, the RMS will vary as function of frequency,
as well as be biased where strong lines are present (or large areas of the map
contain sources). The RMS then needs to be interpolated. In NEMO the code
{\tt ccdstat planes=0} will compute plane based statistics.

\item
Inspecting a typical position-velocity (PV) cut through a cube, makes it apparent
that lines can be detected easily by eye, as their shapes are often related
(this is technically not always true, e.g. recombination lines and molecular lines have
obviously a different origin and work under different ISM conditions). Nonetheless,
by using a well known (and identified) line as a template, it can be moved up and down
along the V axis in a PV diagram, and a cross correlation then results in another method
to identify lines. The experimental codes are in NEMO as {\tt pvcorr}.


\item
A related and derived case of this
cross-correlation technique would be to compute the intensity
weighted velocity within the template, and move this curved line up and down in V and resample
the PV diagram along this shifted line, then add up all the emission along this curved
sample, and this will also be an indication of the detected lines.  Obviously this
method will generate more noise, but will typically have $\sqrt{2}$ more resolution.

Also consider hyperfine structures, coadding their features to increase signal-to-noise.

\item
To balance the noise, a certain amount of smoothing to the data will always help
the identification.


\end{enumerate}

Special care has to be given to the calibration of the lines. Given the nature of
the template and the not well defined velocity of the source, the template line
must have an assumed identity and known rest frequency  ($f_{1R}$). 
With a known Doppler velocity $V_{lsr}$ its sky frequency ($f_1$) 
can then be computed.  Assuming the unknown line is measured ${\Delta f}$ away, one
can show that the rest frequency of the unknown line is given by:


$$
 f_{2R} = f_{1R} + {  {\Delta f} \over { (1-z) }}
$$

\section{Feature Extraction}
\label{s:feature}

Separating signal from noise is one of the most important first steps
in the analysis of an image or a data cube. Many algorithms have
been developed and published over the last 30 years. They basically
can be classified in two main  categories:  thresholding and pattern
matching.

Some recent reviews are available:
Popping et al. 2012,  Hassan (2012) Thesis Section 5.   
% koribalski 2012 - targeted source finding techniques
% Koribalski, B., 2012. Overview on spectral line source finding and visualisati
on. 
% Publications of the Astronomical Society of Australia. xix, 10, 131, 140, 141,
 226
% http://adsabs.harvard.edu/abs/2012PASA...29..359K


Here we review some methods in common use with published papers. 
We are ignoring a very similar cottage industry in the N-body (points)
world. A good recent review and code comparison 
by Knebe et al. (2013) can be found
in {\tt http://arxiv.org/abs/1304.0585}.

ASCL references listed below can be found off {\tt http://ascl.net}, as well
as in ADS {\tt http://adsabs.harvard.edu/}. This is probably the easiest
reference point to find everything related to the listed software.

\subsection{clumpfind}

The Williams et al. 1994 
ClumpFind
\footnote{http://adsabs.harvard.edu/abs/1994ApJ...428..693W}
algorithm has been implemented as {\tt clfind}
in miriad, but is also available in an IDL version. 
{\bf ascl:1107.014}
Now superseded
by {\tt cprops} (CloudProps, Rosolowski and Leroy 2006) {\bf ascl:1102.012}
\footnote{http://arxiv.org/abs/astro-ph/0601706/}.

\subsection{Dendogram}

Also of note is the dendogram approach,



\subsection{duchamp / selavy}

Whiting (2011,2012) for 2D and 3D. Used in ASKAP pipeline. 
{\tt selavy} is the ASKAP 21-cm version. {\bf ascl:1201.011}
\footnote{http://adsabs.harvard.edu/abs/2012MNRAS.421.3242W}

\subsection{blobcat}

Hales et al. 2012.  2D.
{\bf ascl:1208.009}

\subsection{getsources}

Menshchikov et al. 2012. Herschell based. Two dimensional.
\footnote{http://adsabs.harvard.edu/abs/2012A\&A...542A..81M}
Code is not freely available yet, but is expected to be. Example
output made available to us.

\subsection{gaussclumps}

The original stutzki code is also available in GILDAS, as well as in
STARLINK.  It iteratively decomposes a 3D cube into a series of
Gaussian shaped clumps.  It starts by fitting a 3-dimensional clump
locally to the maximum of the input lbv cube. It then subtracts this
clump from the cube, creating a residual map, and then continues with
the maximum of this residual map.  The procedure is repeated until a
stop criterion is met, for instance when the maximum of the residual
maps drops below the 3 sigma level.

{\tt gaussclumps} was developed by Stutzki \& Guesten (1990, ApJ, 356, 513)
who applied it to their M17SW data sets. It was later described and
analyzed in more detail by Kramer et al. (1998, A\&A, 329, 249) who
also applied it to data cubes of various Galactic clouds.  The papers
of e.g. Heithausen et al. (1998) and Simon et al. (2000) give more examples
of the application of gaussclumps to large scale CO maps.

\subsection{imsad}

{\tt imsad} = Image Search and Destroy. 
Implemented in MIRIAD, based on {\tt imfit}, esssentially fits a gaussian
to a peak that sticks out of some plateau.

\subsection{SExtractor}

A widely source extractor (Bertin \& Arnouts 1996), but not so good for blended 
objects.
{\bf ascl:1010.064}

\subsection{SFind}

Hopkins et al. 2002.   Implemented in MIRIAD.

\subsection{MultiFind}
perl script using a series of miriad commands. Kilborn 2001. Written for HIPASS.

\subsection{tophat}

Meyer et al. 2004. For HIPASS. Uses a pattern matching of shapes and cross corre
lation
to find signals.

\subsection{lovedata}

Saintonge 2007. For ALFALFA. Written in IDL. Pattern matching.


\subsection{GammaFinder}

Boyce 2003. Masters thesis Cardiff.  In java. Use gamma function to estimate
noise and reject non-signal

\subsection{1d-1d wavelet}

C++ by FLoer and Winkel 2012.

\subsection{smooth plus clip}

An old method from the WSRT Bosma time, but more formally
introduced by Serra et al. 2011. ATLAS3D.

\subsection{CNHI}

Jurek 2012. for WALLABY project design study. CNHI = 
Characterised Noise HI Source Finder. Pattern matching. Also mentions the
Lutz (1980) one path algorithm, which is used to group regions from
the same source into a single source.


\subsection{DisPerSE}

DisPerSE is open source software for the identification of persistent
topological features such as peaks, voids, walls and in particular
filamentary structures within noisy sampled distributions in 2D,
3D. Sousbie (2013) {\bf ascl:1302.015}
\footnote{http://www2.iap.fr/users/sousbie/web/html/indexd41d.html}
 

Using DisPerSE, structure identification can be achieved through
the computation of the discrete Morse-Smale complex. The software can
deal directly with noisy datasets via the concept of persistence (a
measure of the robustness of topological features). Although developed
for the study of the properties of filamentary structures in the
cosmic web of galaxy distribution over large scales in the Universe,
the present version is quite versatile and should be useful for any
application where a robust structure identification is required, such
as for segmentation or for studying the topology of sampled functions
(for example, computing persistent Betti numbers). Currently, it can
be applied can work indifferently on many kinds of cell complex (such
as structured and unstructured grids, 2D manifolds embedded within a
3D space, discrete point samples using delaunay tesselation, and
Healpix tesselations of the sphere). The only constraint is that the
distribution must be defined over a manifold, possibly with
boundaries.

\subsection{FIVe}

Hacar et al. (2013) discuss a new Friends of Friends (FOF) 
algorithm they dubbed {\tt FIVe} (Friends in Velocity), and
implemented in the R scripting language.
\footnote{http://arxiv.org/abs/1303.2118}


\subsection{Kemper}

A multi-scale approach to automated feature extraction from HI
data cubes. 
So far only published as a research note, Kempker (2005) from RUG.


\subsection{PyFind}

% Paolo Serra <serra@astron.nl>
Serra (2012) at WSRT/ASTRON
has developed this not-yet-named code. PyFind is his current 
placeholder name. 

It is a simple Python code working on FITS files. It smooths
the cube using a set of specified kernels and finds emission at each
resolution by applying a specified threshold. The program applies also
a size filter at each resolution and makes a final mask as the union
of all masks. Two papers in 2012, one on MNRAS and one on PASA, discuss
it.

\subsection{GRID\_core}

Gong and Ostriker (2013),
coming from a more theoretical angle and 
instead of working with densities, developed 
{\tt GRID\_core} that works with
the gravitational potential.
In essense this is simply a more smooth version of
the density. It uses a watershed model (much like ClumpFind)
to define clumpy structures.  If the potential is not directly
available, a poisson solver can be be used to define densities
and with an assumed conversion factor, regions of bound structures
can be defined as well.  {\bf ascl:1302.007}


\subsection{TODO}

Here's some catch phrases that need to be tracked down if they have
relevant code for the source and feature extraction module.

GAIA, MATADOR, AstroMed, SPLAT-VO, Herschel DP, S2Plot, AstroMD. 


\section{Timeline and Deliverables}

In this section we describe the technical approach, the
timeline and the deliverables. Management Plan? 
All in a self-contained package.
2 years, 2.5 FTE is what we have on the whiteboard.

\subsection{Risk Factors}

\begin{enumerate}
\item
Pipeline Archive Integration (Low) : build on local fake archive?
\item
Pipeline Access API (Medium): user will manually have to download files
\item
Some tools too hard (Low): Deliver fewer tools

\end{enumerate}

\section { MARC: Overview }
The ALMA Data Mining Toolkit (ADMIT) is a software suite that operates on
ALMA data cubes. It creates descriptions that summarize to the user key
observational components of the data, as well as a number of "added value"
datasets to facilitate standard scientific analysis and to enable new and
creative ways to derive science from the cubes.  ADMIT is envisioned to
be run as part of the ALMA pipeline to create a new set of products for
the ALMA archive; however it is designed to also be capable of running
as a standalone application operating on user-downloaded cubes, and
re-compute (with perhaps modified parameters) the products created
in the archive.

ADMIT consists of two main components. The core is the infrastructure layer
that defines data structures and programmatic pipeline; provides methods
for I/O, data selection, and computation of scientifically relevant
quantities; extracts and organizes scientific metadata from the cubes; and
defines the application programming interface (API) for higher-level tools.
Built upon the infrastructure layer are the ADMIT CASA Tools that provide
advanced functionality for scientific analysis.

\subsection{ Infrastructure Layer }

% Using present tense here, but perhaps future tense is more appropriate.

ADMIT is fully defined and implemented in a CASA-compatible Python module.
The basic architecture is a pipeline with certain default image operations
(described below), to which the user can add further operations.  ADMIT can
work with images in both FITS and CASA format.  The outputs of the pipeline
are wrapped into a single, self-describing ZIP archive file, which any
ADMIT CASA Tool can parse and manipulate.

ADMIT is runnable from within the CASA Python environment, as individual
commands or a script.  However, for novice users we provide a simple
graphical user interface (GUI) for launching Tools and viewing results.

The core output of ADMIT is an XML file that contains all metadata
extracted from the science data, e.g.  project code, source name, sky
coordinates, ALMA bands used, list of spectral lines covered.  Each ADMIT
operation builds additional detail into the XML file creating a more
comprehensive, scientifically useful description.

In addition, ADMIT provides an infrastructure to operate on multiple
projects, and perform ``the same'' operations on each of those. This
can be done either in parallel, or recursively. It can then extract
information from each project and in essence data mine a large suite
of data, use linked data techniques to visualize the extracted
information and provide new insight on what is common or different in
the sources in these projects.


\subsection{ CASA Tools }

The default pipeline consists of the Summary, Moment, and Line
Identification tools. 

{\it  PETER: for the moment I'm editing these in section \ref{s:tools}}

%a ZIP archive file that contains both the descriptive XML file, and all
%associated - but small - FITS and JPG files.


\section*{References}

\begin{enumerate}
\item
{\bf ASTUTE wiki:} {\tt  http://carma.astro.umd.edu/wiki/index.php/AStute}

\item
{\bf more to come...:}

\end{enumerate}

\end{document}

