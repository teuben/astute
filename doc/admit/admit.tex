%% IMAGE I/O
%% 17-oct-2011 First Draft     - Peter Teuben

\documentclass[preprint]{aastex} % AASTeXv5.0
\usepackage{carma_memo}

\begin{document}
\carmamemo{999} 	

\title{ADMIT: ALMA Data MIning Toolkit}

\author{Peter Teuben}
\affil{University of Maryland}

\begin{abstract}

An overview of ADMIT (ALMA Data Mining Toolkit) V1 is given. This is
the outcome of an ALMA Development Study awarded to Mundy and Varshney
(2012), in preparation for an ALMA Development Project, due Augus 16,
2013.

This memo describes the technical aspects of ADMIT, some working, and
some still a mockup. All working and example codes are available in a
CVS module named {\tt astute}, including this memo.

ADMIT is targeted at both a novice user (via a convenient GUI), as
well as an experienced user (python toolkit within the CASA
framework), and from the ground up is designed to be extensible via
plugins.

{\bf CAVEAT}: ADMIT was formerly known as ASTUTE, and you will still
find the latter name where the former is meant.


\end{abstract}


\ChangeRecordBegin
\addrevision{0.1}{2013-Aug-11}{P. Teuben}{}
{Relevant material transferred from image-io memo and adopted ASTUTE to ADMIT}
\ChangeRecordEnd


\section{Introduction}

ADMIT (ALMA Data MIning Toolkit)\footnote{formerly known as ASTUTE. 
This document will still use the name ASTUTE in places where ADMIT is meant.}
is a toolkit that operates on ALMA data, and from existing data cubes, 
creates a number of ``added value'' datasets that summarizes to the user
what science they can expect from this data. They are wrapped into what
we call the {\tt admit.zip} file, and contain both the descriptive XML
file, and all associated -- but small -- FITS and JPG files.

The ALMA pipeline ingests visibility data (in ASDM format) and after calibration
and mapping produces a series of image datacubes, one for each band (currently 
NB=4 for ALMA, with 3840 channels per band), and one for each distinct
source (usually NS=1, but if that applies to the science 
goals the project has been set up, with multiple sources are possible). If
single dish has been added, and/or a source contains multiple fields,
these will be combined into one single source image datacube.


\section{Outline}

The following plugins are considered to be part of ADMIT:

\begin{enumerate}

\item
{\bf Summary}: this creates a container in which the number of projects (NP),
sources (NS), bands (NB) and lines (NL) will (eventually) be set. This, and 
associated table describing these, will determine the workflow in the
ADMIT pipeline. In each subseqent step this container (maintained in XML) will
be updated such that at activating ADMIT in a project, it will quickly
present a summary of everything that has been gathered and computed.

\item
{\bf Line Identification}


\item
{\bf Moment Maps}


\item
{\bf Source Extraction}


\item
{\bf Source Characterization}


\end{enumerate}


\begin{verbatim}

1. ALMA and the Context of this study

ALMA overview 2-3 slides
Basic data is fourier components Pipeline creates images
CASA software package
CASAViewer
Capacity of instrument to produce data
Target Users
  novice
  experienced
  expert Goals:
  Create a metadata product which enhances the
     user experience by giving statistucs and
     overviews of the data
  Create metadata products which facilitates the
     user's ability to visualize the data and
     analyze the data
  Enable new methods for doing science with the data
  Create an infrastructure which encourages user
     contributed methods/codes and allows

Study is perparation for a Project
Constraints: need to work within the system

2. ALMA Arichive, Data flow and metadata flow

Data flow -- from correlator at site
             to Santiago
         to pipeline
         to archive and science centers
Archive structure
    Diagram and explanation
    Ground rule for our project -- not ours to change

Scale of archive as a science treasure trove

Modes of use:
   Current user interaction
       retrieve observations for own project
       retrieve archival observations on desired object
   Future science mining activities
      pulling together multiple object samples
      gathing all data on single objects

Implimentation:
A. Create metadata as tail end of imaging pipeline
    metadata goes into archive and is served from there
B. User improves images and recreates metadata
C. User runs specialized programs to create new
    metadata for specific scientific uses.
\end{verbatim}

\section{Code Technical Material}

In this section we describe

\subsection{Scenarios}

Several scenarios are given how ADMIT can be used.  For nomenclature, we use {\bf P} for
the projects, {\bf S} for a source, {\bf B} for the bands (spectral windows), and 
{\bf L} for the identified and unidentified lines. They count to 
NP projects, with NS sources, NB bands and NL lines.
These spectral window bands can be independant sections on the frequency axis, 
and can contain both line and continuum radiation. The continuum radiation is 
allowed to have a spectral 
index (or in worst case: a true parameterized shape, e.g. black body)

\subsubsection{Pipeline: run a single project}

This would be the mode in which ASTUTE runs in the archive, i.e. from one or more spectral
windows (ALMA: NP=1, NS$\ge$1, NB=4, NL$\ge$0)\footnote{do we need to say something about 
polarization here?}

\footnotesize
\begin{verbatim}
# code0.py:

import astute

as = astute.Astute()

\end{verbatim}
\normalsize


\subsubsection{User: simple project archive pull}

This example is the case where a user has selected one project (NP=1)
in which one source was present (NS=1), but 4 associated 
spectral windows (NB=4). In these 9 spectral lines were detected (NL=9),
resulting in 9 data-cubes, with associated moment-0,1,2 maps in FITS
and JPG format. An additional clump identification was made, available
in tabular format.

\footnotesize
\begin{verbatim}
# code1.py:
# use case: start with an archive query, and grab data as well, as needed
#

import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

projects = ar.query('gal && line(CO) && z<0.2 && T>1')
np = size(projects)

for p in projects:
    as.setdir(p.name)                 # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:                 # project 'p' and a series of 'c' cubes
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsubsection{User: code2}

In this case the user has decided to select another undefined line

\footnotesize
\begin{verbatim}
# code2.py:
# In a currently astute activated code
#


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

#  search here and below for astute.xml files
#  optionally a query for science, e.g. only CO lines
projects = as.query_dir('.','line(CO)')
np = size(projects)

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsubsection{User: code3}

In this example the user has harvested a series of CO cubes from
an archive query  that resulted in many projects (NP=120), 
from these CO cubes, clumps have been identified using a modified
algorithm from the one the ASTUTE pipeline had used, and the slope
of the mass  spectrum is correlated against the mass of the galaxy.

\footnotesize
\begin{verbatim}
# code3.py:
#
# Example code3
#
# use case here is that you have a series of astute enabled directories
# in which you've done all kinds of interesting work, but results
# are stored back in "astute" (astute.xml)
#


import astute

as = astute.Astute()

projects = as.query_dir('.','line(CO)')
np = size(projects)

#  we want to plot L vs. S
s=[]
l=[]

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper project directory
    s.append(p.getpar("s"))
    l.append(p.getpar("l"))
    #
#

\end{verbatim}
\normalsize


\subsection{Archive Access}

Some of ASTUTE's functionality depends on how well it hooks into the archive.

Is it going to depend on VO services? The SIAV2 data access protocol
will handle multi dimensional image data cubes, an early draft is expected
by Aug 16, and a SIAV2 prototype / reference implementation is planned to 
be demoed at the IVOA interop during ADASS this year (Tody, priv.comm.)

Or is it going to depend on a direct API into the archive? 

By default ASTUTE runs and the ``big'' cubes are locally present. But if derived
products are not present in the working directory, a procedure will have to 
extract this from the archive.

\subsection{admit.zip}

The plan is to wrap our XML file and any associated files (
Python has good methods to manage zip files, maintenance will be done in 
a temporary (?) subdirectory per project. Code examples are in azip.py



\section{Related Technologies}

There are a few notes on existing technologies that 
impact further development of ADMIT\footnote{where missing, the wiki probably has 
links and more info}:

\begin{enumerate}

\item
CASA is using Qt (not GTK+) as their GUI. Our mockup version with glade is GTK+ based,
so we'll have to switch as soon as we go and do the real thing.

\item
The now working VLA pipeline is written in casapy, and would be worthwhile studying.

\item
The {\tt ParselTongue} package (ascl:1208.020, Kettenis et al, 2006ASPC..351..497K)
is a python package that lets you run AIPS (and other) tasks. 
They have many methods that resemble what ADMIT has to do. In particular, having
an option to wrap other common packages (e.g. MIRIAD) in ADMIT, and/or have an option
to add any package (AIPS, OBIT, NEMO, ....) would be a powerful addition
that allows flexible python scripting at a high level.

\item
Related to the previous two item: lets not forget our own CADRE (ascl:1303.017, Friedel 2013)

\item
CASA's own {\tt casaviewer} is a client/server program which should be at least as good
as {\tt ds9} to interactively view image cubes. ADMIT should make use of that, but also
give an option to use {\tt ds9} if a user already has this loaded and wants to compare/blink  
from another package.

\item
The {\tt datasplat} interface to Splatalogue (Kent \& Remijan) is 
a good example that we can expand on/ collaborate
on adding line identifications to ADMIT, although we need a number of optional preprocessors
to extract a good {\it line-strength} vs. {\it frequency} table that is the input 
to {\tt datasplat}. One of the strengths of ADMIT is that this extraction method is
flexibly set or determined via heuristics.

\item
The linked data concept from the python-based 
{\tt glue} project is highly suited to build tasks in our environment.

\item
The visualization tools in the python-based {\tt yt} project are impressive, although
these are based on theory. Bolatto's N253 CO data cube received much attention in the
literature with Rosalowski assisted in producing a 3D visualization of the clumps
in N253.



\end{enumerate}


\section{Line Identification}


The topic of detection and identification of spectal
lines in a complex source datacube is a complex one,
of which we offer several solutions. 

\begin{enumerate}

\item
Measuring a robust RMS in a channel (the intent being that the source is removed from the 
RMS calculation), and comparing this to the peak value in a channel gives a good 
indication of a line detection. For channels that do not contain any signal, 
the PEAK/RMS will typically be around 3 or 4, depending on the size of the map.
Note that for wide spectral windows, the RMS will vary as function of frequency,
as well as be biased where strong lines are present (or large areas of the map
contain sources). The RMS then needs to be interpolated. In NEMO the code
{\tt ccdstat planes=0} will compute plane based statistics.

\item
Inspecting a typical position-velocity (PV) cut through a cube, makes it apparent
that lines can be detected easily by eye, as their shapes are often related
(this is technically not always true, e.g. recombination lines and molecular lines have
obviously a different origin and work under different ISM conditions). Nonetheless,
by using a well known (and identified) line as a template, it can be moved up and down
along the V axis in a PV diagram, and a cross correlation then results in another method
to identify lines. The experimental codes are in NEMO as {\tt pvcorr}.


\item
A related and derived case of this
cross-correllation technique would be to compute the intensity
weighted velocity within the template, and move this curved line up and down in V and resample
the PV diagram along this shifted line, then add up all the emission along this curved
sample, and this will also be an indication of the detected lines.  Obviously this
method will generate more noise, but will typically have $\sqrt{2}$ more resolution.

\item
To balance the noise, a certain amount of smoothing to the data will always help
the identification.


\end{enumerate}

Special care has to be given to the calibration of the lines. Given the nature of
the template and the not well defined velocity of the source, the template line
must have an assumed identity and known rest frequency  ($f_{1R}$). 
With a known Doppler velocity $V_{lsr}$ its sky frequency ($f_1$) 
can then be computed.  Assuming the unknown line is measured ${\Delta f}$ away, one
can show that the rest frequency of the unknown line is given by:


$$
 f_{2R} = f_{1R} + {  {\Delta f} \over { (1-z) }}
$$

\section*{References}

\begin{enumerate}
\item
ASTUTE wiki:  http://carma.astro.umd.edu/wiki/index.php/AStute

\item

\end{enumerate}

\end{document}

