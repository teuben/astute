%% ADMIT.TEX
%% 11-aug-2013 astute -> admit

\documentclass[preprint]{aastex} % AASTeXv5.0
\usepackage{carma_memo}

\begin{document}
\carmamemo{999} 	

\title{ADMIT: ALMA Data MIning Toolkit}

\author{Peter Teuben}
\affil{University of Maryland}

\begin{abstract}

An overview of ADMIT (ALMA Data Mining Toolkit) V0.5 is given. This is
the outcome of an ALMA Development Study awarded to Mundy and Varshney
(2012), in preparation for an ALMA Development Project, due August 16,
2013. Cut and paste at your hearts content.

This memo describes the technical aspects of ADMIT, some working, and
some still a mockup. All working and example codes are available in a
CVS module named {\tt astute}, including this memo.

% ==== a bit about the what it is
ADMIT operates on ALMA data. Within the ALMA pipeline environment
it will first construct ``data added value'' components that enables
an archive user to then quickly acess the scientific value of the data,
before downloading the actual data. ADMIT will then assist in
recomputing selected results, and optionally finetune them for the user.


ADMIT consists of two main components. The core is the infrastructure layer
that defines data structures and programmatic pipeline; provides methods
for I/O, data selection, and computation of scientifically relevant
quantities; extracts and organizes scientific metadata from the cubes; and
defines the application programming interface (API) for higher-level tools.
Built upon the infrastructure layer are the ADMIT CASA Tools that provide
advanced functionality for scientific analysis.

%===

ADMIT is targeted at both a novice user (via a convenient GUI), as
well as an experienced user (Python toolkit within the CASA
framework), and is designed to be extensible via plugins.

\bigskip
{\bf CAVEAT}: ADMIT was formerly known as ASTUTE, and you will still
find the latter name where the former is meant.


\end{abstract}


\ChangeRecordBegin
\addrevision{0.1}{2013-Aug-11}{P. Teuben}{}
{Relevant material transferred from image-io memo and adopted ASTUTE to ADMIT}
\addrevision{0.2}{2013-Aug-16}{P. Teuben}{}
{material for ALMA Development Proposal}
\ChangeRecordEnd


\section{Introduction}

ADMIT (ALMA Data MIning Toolkit)\footnote{formerly known as ASTUTE. 
This document will still use the name ASTUTE in places where ADMIT is meant.}
is a toolkit that operates on ALMA data, and from existing data cubes, 
creates a number of ``added value'' datasets and descriptions 
that summarize to the user
what science they can expect from these data. Some of these data
are then wrapped into what
we call the {\tt admit.zip} file, and contain both the descriptive XML
file, and all associated -- but small -- FITS and JPG files.

The ALMA pipeline ingests visibility data (in ASDM format) and after calibration
and mapping produces a series of image datacubes, one for each band (currently 
NB=4 for ALMA, with NC=3840 channels per band), and one for each distinct
source (usually NS=1, but depending on the science 
goals the project has established, multiple sources are possible). If
single dish is available, and/or a source contains multiple fields,
the ALMA pipeline will combine them into one single source 
mosaiced image datacube.

ADMIT will then prepare a small summary for the archive, enabling users
to get an overview before the actual large data has been downloaded.

ADMIT contains two components: the infrastructure and the plugins.


\section{ADMIT Tools outline}

\label{s:tools}

Here we outline a number of ADMIT Tools (``plugins'').  Although they
are presented in a serial fashion, there is some advantage of a
waterfall type model, where for example you redo the statistics based
on the knowledge where the signal is, which was derived from later steps. At
each step new meta-data is gathered that describes the data.

At each stage ADMIT will return data structures in Python, for easy
integration in casapy or any other Python module accessible by
CASA. Some examples of this are given in code fragments in the 
Section~\ref{s:scenarios}.

First we will size up the arrays and tables used in ADMIT, with the 
following dimensions:

\begin{enumerate}

\item {\bf NP:}
Number of projects. In the ALMA pipeline this is of course one, but ADMIT
can also supervise and datamine many projects, in which case this number
is relevant and larger than 1.

\item {\bf NS:}
Number of sources per project. This is usually 1, but it can be more.

\item {\bf NB:}
Number of independant correlator bands, per project/source.

\item {\bf NC:}
Number of channels in a band.

\item {\bf NL:}
Number of lines detected accross all bands.

\item {\bf NV:}
Number of velocity channels in a line cube

\item {\bf NY:}
Number of Y (DEC) pixels in a cube

\item {\bf NX:}
Number of X (RA) pixels in a cube

\item {\bf NF:}
Number of features detected in a line cube

\end{enumerate}

In the data structure array notation we will use below, the C-convention is used, 
meaning the first indices are the ones on top of the hierarchy, i.e. slowest moving.
For example $cube[NP][NS][NV][NY][NX]$ would be a set of $(NX,NY,NV)$ cubes, 
for $NP$ projects, but in each project one can have a different number ($NS$) of
sources.

Now we will describe a few of the modules in the ADMIT pipeline.


\subsection{Data Summary}

This creates a container in which the number of projects (NP),
sources (NS), bands (NB), lines (NL) and features (NF) will be set. 
This, and 
associated tables describing these, will determine the workflow in the
ADMIT pipeline. In each subseqent step this container (maintained in XML) will
be updated such that at activating ADMIT in a project, it will quickly
present a summary of everything that has been gathered and computed. 

One of the first components of this data summary is a simple cube
statistics table with properties such as min, max, mean, 
robust median, rms per channel,
which can be the input for the next step of line identification.

$$
cube[NP][NS][NB][NC][NY][NX] \rightarrow stats[NP][NS][NB][NC]
$$
where 
$$
     cube = \{ intensity [, mask] \}
$$
and
$$
stats = \{min,max,rms,mean,median,...\}
$$

A {\it (robust) median} is useful, but makes the procedure more expensive.


\subsection{Line Identification}

A {\it line-strength} vs. {\it frequency} table is the essential input
into a line identification procedure (see e.g. datasplat).
The input can be as simple as the previously computed cube statistics, 
but aided by cross-correlation techniques in a PV diagram, or the cube itself,
aided by where the emission is (see also Figure~\ref{fig:pvcorr4}
and Section~\ref{s:line}).
The output will be a series of identified and unidentified lines accross the different
bands, NL in total, therefore reducing a dimension of NB*NC to NL.
This is a simple table of 
{\it line, frequency, detection probability}, with NL entries.

$$
    line[NP][NS][NL]
$$
with
$$
    line = \{name\_id, frequency, probability\}
$$


\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{pvcorr4.eps}
\caption{Line identification can be tremendously improved in a PV diagram
by cross-correlating the signal in the V direction. Compare the more traditional
noisy peak/rms plot in the middle (in white) 
with the smooth cross-correlation technique on
the right (in red). The two vertical lines denote zero and a 3 sigma clipping 
above which we deem the line significant.}
\label{fig:pvcorr4}
\end{figure}


\subsection{Line Cubes}

Based on the line identification  line cubes can be extracted,
with now the third axis in doppler velocity space, to ensure
that we can compare the different lines and molecules on a channel
by channel basis.

$$
     linecube[NP][NS][NL][NV][NY][NX]
$$
where 
$$
     linecube = \{ intensity [, mask] \}
$$

Binning is an option, if signal to noise needs to be increased without
loss of spectral resolution, but the exact gridding needs to be 
the same.
Depending on the number of lines in the bands,
keeping only the line cubes can significantly cut down disk usage.
We denote the number of velocity channels by NV. 

For some interesting point in the moment 0 map (the peak?), a spectral profile
is stored by ADMIT to guide the user to the new 3rd dimension.

\subsection{Moment Maps}

In the previous step NL line cubes have been cut from the NB band cubes,
with the option to re-bin the cubes.
Cubes will now be labeled in doppler velocity space, with the intent
that all cubes have the same gridding applied.  Simple moment 0, 1 and 2 maps
are created with a selected method for where the emission is. This can 
be based on a simple RMS cutoff, but also
more sophisticated methods that include local smoothing to
increase the signal to noise to create a more robust mask that is then used
for the creation of the moment maps. Moment maps are exported to {\tt admit.zip}
for visual inspection.

$$
    map[NP][NS][NL][NY][NX]
$$
with
$$
    map = \{mom0, mom1, mom2, mask\}
$$

Moment maps can also be produced using parametric shapes, e.g. a gaussian fit, which
would then store the total emission, mean velocity and width of the spectrum at each
location instead of the classic moments.

\subsection{Feature Extraction}

Many sophisticated feature (``source'') extraction routines are available from the community
(clumpfind, dendrograms, gaussclumps, a more complete is given in Section~\ref{s:feature}),
which essentially produce
a table of properties for each feature. This table is stored as a VO Table 
in {\tt admit.zip}, with NF entries for each  detected feature. Features
are allowed to be hierarchical in nature, such as for dendrograms. 
The essential data structure is:

$$
   feature[NP][NS][NL]
$$
with
$$
   feature = \{X, Y, V, I, shape\}
$$
where {\it shape} is a more general description containing (amongst) the size
and orientation, and possibly higher order moments, at a given location
{\it X,Y,V} in a cube with integrated emission {\it I}. 


\subsection{Feature Characterization}

Since for each of the NL line cubes, a catalog of features is available,
characterization of the emission accross different molecules/lines is now 
possible on a scale never been seen before. Principle Component
Analysis (PCA) and an Overlap Integral (OI) are two ways to quantify
and visualize this. A linked data tool (e.g. {\tt glue}) can be 
of tremendous help here.

\subsection{Overlap Integral}

One way to get an overview of the type of emission accross all lines
is the overlap integral.
For each detected line one assigns a bit in an integer, and then logically ORs
them accross all lines, giving a map of a bit-mask of the emission present
at this location. One can do this in a map, or even in a cube, and study
in which regions certain lines are present and/or absent in relation
to other lines.

\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{n253_overlap8.eps}
\caption{Overlap integral: on the right the moment 0 map of one particular line,
on the left the combined overlap integral - valued 0..255 - of 8 lines, each
using a single bit in an 8 bit integer. As can be seen, most of the peaks
have all 8 lines present. There are some recombination lines present in this
cube, which clearly have a different spatial distribution.}
\label{fig:overlap}
\end{figure}



\subsection{Principal Component Analysis}

An extension of the overlap integral would be to not use a simple on/off bitmask,
but create a vector containing the full strength for each line, and using
a Principle Component Analysis find out the basis vectors of what describes
the emission, including how many different regions there are, and what they
represent.

See Meier \& Turner (2005), where they took eight molecules 
observed with OVRO of IC 342.

\subsection{Saliency}

Saliency in a dataset is based on segmenting the emission and assigning
a description vector to the features found. Description vectors can be
picked from a number of properties of the emission. In vision research
(e.g. Ip et al, 2013)
a popular one is a color histogram, which enables a very fast
cataloging of the types of features, as well as finding the ones similar
to a selected one.


In astronomical data cubes, finding the best description vector is key
to finding the interesting features.  Extending the experience in
vision research, where a 3-channel RGB histogram was used, we can see
our NL lines as a ``color'' and assign a description vector based on 
this to efficiently catalog the types of regions that exist in large
datacubes.
Another example is a description
vector based on the moments of inertia, or those based pon a moment
0,1,2 map.  For a selected object, one can then ask what other objects
have similar description vectors, as in a PCA, and perform a cluster
analysis to decipher how many kinds of objects there are given the
description that was selected.



\section{Scenarios}

\label{s:scenarios}

Several scenarios are given how ADMIT can be used.  For nomenclature, we continue
to use {\bf P} for
the projects, {\bf S} for the sources, {\bf B} for the bands (spectral windows), 
{\bf L} for the identified and unidentified lines, and {\bf F} for the extracted
features in a line cubes. They count to 
NP projects, NS sources, NB bands, NL lines and NF features.
These spectral window bands are independent sections on the frequency axis, 
and can contain both line and continuum radiation. NL will then refer to all the
lines in all the NB bands.
The continuum radiation is allowed to have a spectral 
index (or in worst case: a true parameterized shape, e.g. black body).


In each of the 4 code scenarios we will expose the enourmous power of the
ADMIT toolkit in handling such a new situation.

\subsection{Pipeline: initialize a single project}

This would be the mode in which ADMIT runs at the tail end of the ALMA pipeline
in the archive, i.e. from one or more spectral
windows (ALMA: NP=1, NS=1, NB=4, NL$\ge$0)\footnote{do we need to say something about 
polarization here? Not on 2 yr timescale?} 
the previously described steps produce ancillary data for the
{\it admit.zip} file.

\footnotesize
\begin{verbatim}
# code0.py:

import astute

as = astute.Astute()               # initialize from the current directory
as.pipeline(0)                     # run a default pipeline
as.save()                          # save everyting in admit.zip

\end{verbatim}
\normalsize


\subsection{User: query archive for a single project}

This example is the case where a user has selected one project (NP=1)
in which one source was present (NS=1), but 4 associated 
spectral windows (NB=4, NC=3840). In these cubes 19 spectral lines 
were detected (NL=19),
resulting in 19 line data-cubes, with associated moment-0,1,2 maps in FITS
and JPG format. An additional feature identification was made, available
in tabular format.  The user visualizes them, and extracts the 19
data cubes, and recomputes moment maps with a slightly less conservative
setting of the clipping, and recomputes feature detection and compares
them with the ones provided previously by ADMIT.

\footnotesize
\begin{verbatim}
# code1.py:


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

projects = ar.query('gal && line(CO) && z<0.2 && T>1')
np = size(projects)

for p in projects:
    as.setdir(p.name)                 # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:                 # project 'p' and a series of 'c' cubes
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsection{User: recompute properties using different algorithms}

In this case the user has decided to select an undefined line, identify it, 
and add this to the ADMIT database, in addition, suggest a feedback to the ALMA data archive
(speculative) for this particular project.
Recomputing and re-correlating data is an essential ingredient of ADMIT.

\footnotesize
\begin{verbatim}
# code2.py:


import astute

line=[-200, 200, 10]

as = astute.Astute()
ar = astute.Archive()

#  search here and below for astute.xml files
#  optionally a query for science, e.g. only CO lines
projects = as.query_dir('.','line(CO)')
np = size(projects)

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper directory
    nc = size(p.cubes)
    for c in p.cubes:
        x = p.grab('x')
        if x.hasline('co'):
            f = p.grab('fits')
            as.importfits(f)         # this writes a MS
            as.regridvel(f,line)
            rms = x.get_rms(line)
            as.moment0(f)
        #
    #
#

\end{verbatim}
\normalsize


\subsection{User: query archive for many projects and cross-correlate clump properties}

In this example the user has harvested a series of CO cubes via
an archive query that resulted in many object/projects (NP=121).
From these CO cubes, clumps have been identified using a modified
algorithm from the one the ADMIT pipeline had used, and the slope
of the mass spectrum is correlated against the mass of the galaxy.
This really shows some of the more advanced power in ADMIT
of gathering datasets (be it via an archive query, or assembly by
hand or other means), and datamining these dataset with ADMIT tools.
Under the hood these tools are not limited to CASA tools and tasks, but
can also be taken from external packages with minimal work (cf.
ParselTongue).

\footnotesize
\begin{verbatim}
# code3.py:

import astute

as = astute.Astute()

projects = as.query_dir('.','line(CO)')
np = size(projects)

#  we want to plot L vs. S
s=[]
l=[]

#  loop over the ones found, p is a container for lots of ASTUTE goodies
for p in projects:
    as.setdir(p.name)                # move into the proper project directory
    s.append(p.getpar("s"))
    l.append(p.getpar("l"))
    #
#


\end{verbatim}
\normalsize

\subsection{User: A multi-source investigation}

In this use case scenario the user has already downloaded a number
of galaxies from the ALMA archive, and made sure they include a CO
line cube and are ADMIT enabled.
The CO distribution is clumpy and resolved, but is
generally in a plane where the clumps are rotating around the center
of the galaxy. The hypothesis is that the clump mass spectrum slope is
dependent on the total mass of the galaxy, which in turn depends on
the measured rotation speed at some maximum measured distance. In this
particular case the user decides to recompute the clumps using a ``clumpfind''
algorithm with a more conservative setting of the noise. The resulting
spectrum is computed and a slope fitted.  For the mass, a simple
rotation curve is fitted, and the mass is derived from the
outermost radius and rotation speed.  A scatter plot is created of
the slope vs. log(M).

Here is some sample ADMIT code how you could do this:

\newpage

\footnotesize
\begin{verbatim}
import admit, numpy, scipy.stats, math     # grab some needed modules

adm = admit.ADMIT()                        # initialize ADMIT

projects = adm.query_dir('.','line(CO)')   # look for CO cubes
np = size(projects)                        # see how many we got


m_list = []                # accumulate masses of the galaxies
a_list = []                # accumulate slopes of the clump mass spectra

for p in projects:
  adm.setdir(p.dirname)    # move into the proper project directory
  for c in p.cubes:        # loop over cubes (should be only 1 !!!)

    rms = 3 * p.rms                        # set a new rms for clumpfind
    f  = adm.feature(c,'clumpfind',dt=rms) # recompute clump list
    cm = f.getcol('mass')
    logm = numpy.log10(cm)
    (x,y) = numpy.histogram(logm)
    slope = scipy.stats.linregress(x,y[1:])[0]
    a_list.append(slope)                   # Mass Spectrum Slope

    m1 = c.moment(1)                           # reference to the moment-1 map
    rc = adm.rotcur(m1)                        # fit a rotation curve 
    rad = rc.getcol('r')                       # grab array of radii
    vel = rc.getcol('vrot')                    # grab array of rotation speeds
    rmax = rad.max()                           # radii, in kpc
    vmax = vel.max()                           # in km/s, inclination corrected
    dist = rc.getpar('distance')               # distance, in mpc
    m_list.append(math.log10(vmax*vmax*rmax))  # log(Galaxy_Mass), ignore G
  #
#
m = numpy.array(ml)
a = numpy.array(al)
adm.plot2d(m,a)                            # show on a plot


\end{verbatim}
\normalsize

\newpage

\begin{figure}[ht]
% \epsscale{0.60} 
\plotone{mock1.eps}
\caption{Example GUI: On the left from top to bottom the Inspector, Moment, 
Morphological Analysis, Principal Component Analysis, and Spectral Cutout
actions for the data selected in the middle panel. On the top row tab's are
visible for a number of collected sources. NOTE: the middle panel will be replaced
with real figures from ADMIT/matplotlib}
\label{fig:gui1}
\end{figure}



\section{ADMIT Pipeline}


ADMIT consists of two components: an infrastructure to run
and maintain a pipeline, and a series of ADMIT tools. The pipeline
infra-structure is loosely based on the MIS pipeline (Pound \& Teuben, 2011?),
where the tools are simply CASA tools working within the ADMIT environment.


\section{Archive Access}

Some of ADMIT's functionality depends on how well it hooks into the archive,
and until that is open, ADMIT will depend on manually downloading the cubes
for (re)processing.  By default, ADMIT can only show a graphic summary of
a dataset, and will need large cubes

Is it going to depend on VO services? The SIAV2 data access protocol
will handle multi dimensional image data cubes, an early draft is expected
by Aug 16, and a SIAV2 prototype / reference implementation is planned to 
be demoed at the IVOA interop during ADASS this year (Tody, priv.comm.)

Or is it going to depend on a direct API into the archive?  


\subsection{admit.zip}

We wrap our XML data (tables etc.) and any associated files (all meant to be small)
into the {\tt admit.zip} file.
Python has good methods to manage zip files, maintenance will be done in 
a temporary (?) subdirectory per project. Code example in azip.py

There are two types of {\tt admit.zip} files: the one per project, that describes
all data and science products, and one that describes projects.

Possible hierarchy we find in {\tt admit.zip}: (products with * are large and don't come
with ADMIT and have to be recomputed and refetched from archive)
\footnotesize
\begin{verbatim}
ProjectName
    Nsource
    SourceName1
       Ra
       Dec
       Vlsr
       Nband
       Band1
           BandCube (*)
           FreqSetup
           CubeStats
       Band2
       ...
       Nline
       VelSetup
       Line1
           LineCube (*)
           PeakProfile
           Mom0
           Mom1
           Mom2
           FeatureList
       Line2
       ...
       OverlapIntegral
    Sourcename2
    ...    
\end{verbatim}
\normalsize


\section{Related Technologies}

There are a few notes on existing technologies that 
impact further development of ADMIT\footnote{where missing, the wiki probably has 
links and more info}:

\begin{enumerate}

\item
CASA is using Qt (not GTK+) as their GUI. Our mockup version with glade is GTK+ based,
so we'll have to switch as soon as we go and do the real thing.

\item
The now working VLA pipeline (Chandler) 
is written in casapy, and would be worthwhile studying in more detail.

\item
The {\tt ParselTongue} package (ascl:1208.020, Kettenis et al, 2006ASPC..351..497K)
is a python package that lets you run AIPS (and other) tasks. 
They have many methods that resemble what ADMIT has to do. In particular, having
an option to wrap other common packages (e.g. MIRIAD) in ADMIT, and/or have an option
to add any package (AIPS, OBIT, NEMO, ....) would be a powerful addition
that allows flexible python scripting at a high level.

\item
Related to the previous two item: lets not forget our own CADRE (ascl:1303.017, Friedel 2013)

\item
CASA's own {\tt casaviewer} is a client/server program which should be at least as good
as {\tt ds9} to interactively view image cubes. ADMIT should make use of that, but also
give an option to use {\tt ds9} if a user already has this loaded and wants to compare/blink  
from another package. Plans exist to replace {\tt casaviewer}.

\item
The {\tt datasplat} interface to Splatalogue (Kent \& Remijan) is 
a good example that we can expand on/ collaborate
on adding line identifications to ADMIT, although we need a number of optional preprocessors
to extract a good {\it line-strength} vs. {\it frequency} table that is the input 
to {\tt datasplat}. One of the strengths of ADMIT is that this extraction method is
flexibly set or determined via heuristics.

\item
The linked data concept from the Python-based 
{\tt glue} project is highly suited to build tasks in our environment.

\item
The visualization tools in the python-based {\tt yt} project are impressive, although
these are based on theory. Bolatto's N253 CO data cube received much attention in the
literature with Rosalowski assisted in producing a 3D visualization of the clumps
in N253.

\item
{\tt ipython} has a {\it notebook} capability, much like Mathematica's Notebook,
which allows one to play back (think verifyability). In another way one could see
this as parallel to the {\tt admit.zip} file. notebooks are stored in JSON
format.


\end{enumerate}


\section{Line Identification}
\label{s:line}

The topic of detection and identification of spectal
lines in a complex source datacube is a complex one,
of which we offer several solutions. 

\begin{enumerate}

\item
Measuring a robust RMS in a channel (the intent being that the source is removed from the 
RMS calculation), and comparing this to the peak value in a channel gives a good 
indication of a line detection. For channels that do not contain any signal, 
the PEAK/RMS will typically be around 3 or 4, depending on the size of the map.
Note that for wide spectral windows, the RMS will vary as function of frequency,
as well as be biased where strong lines are present (or large areas of the map
contain sources). The RMS then needs to be interpolated. In NEMO the code
{\tt ccdstat planes=0} will compute plane based statistics.

\item
Inspecting a typical position-velocity (PV) cut through a cube, makes it apparent
that lines can be detected easily by eye, as their shapes are often related
(this is technically not always true, e.g. recombination lines and molecular lines have
obviously a different origin and work under different ISM conditions). Nonetheless,
by using a well known (and identified) line as a template, it can be moved up and down
along the V axis in a PV diagram, and a cross correlation then results in another method
to identify lines. The experimental codes are in NEMO as {\tt pvcorr}.


\item
A related and derived case of this
cross-correlation technique would be to compute the intensity
weighted velocity within the template, and move this curved line up and down in V and resample
the PV diagram along this shifted line, then add up all the emission along this curved
sample, and this will also be an indication of the detected lines.  Obviously this
method will generate more noise, but will typically have $\sqrt{2}$ more resolution.

Also consider hyperfine structures, coadding their features to increase signal-to-noise.

\item
To balance the noise, a certain amount of smoothing to the data will always help
the identification.


\end{enumerate}

Special care has to be given to the calibration of the lines. Given the nature of
the template and the not well defined velocity of the source, the template line
must have an assumed identity and known rest frequency  ($f_{1R}$). 
With a known Doppler velocity $V_{lsr}$ its sky frequency ($f_1$) 
can then be computed.  Assuming the unknown line is measured ${\Delta f}$ away, one
can show that the rest frequency of the unknown line is given by:


$$
 f_{2R} = f_{1R} + {  {\Delta f} \over { (1-z) }}
$$

\section{Feature Extraction}
\label{s:feature}

Separating signal from noise is one of the most important first steps
in the analysis of an image or a data cube. Many algorithms have
been developed and published over the last 30 years. They basically
can be classified in two main  categories:  thresholding and pattern
matching.

Some recent reviews are available:
Popping et al. 2012,  Hassan (2012) Thesis Section 5.   
% koribalski 2012 - targeted source finding techniques
% Koribalski, B., 2012. Overview on spectral line source finding and visualisati
on. 
% Publications of the Astronomical Society of Australia. xix, 10, 131, 140, 141,
 226
% http://adsabs.harvard.edu/abs/2012PASA...29..359K


Here we review some methods in common use with published papers. 
We are ignoring a very similar cottage industry in the N-body (points)
world. A good recent review and code comparison 
by Knebe et al. (2013) can be found
in {\tt http://arxiv.org/abs/1304.0585}.

ASCL references listed below can be found off {\tt http://ascl.net}, as well
as in ADS {\tt http://adsabs.harvard.edu/}. This is probably the easiest
reference point to find everything related to the listed software.

\subsection{clumpfind}

The Williams et al. 1994 
ClumpFind
\footnote{http://adsabs.harvard.edu/abs/1994ApJ...428..693W}
algorithm has been implemented as {\tt clfind}
in miriad, but is also available in an IDL version. 
{\bf ascl:1107.014}
Now superseded
by {\tt cprops} (CloudProps, Rosolowski and Leroy 2006) {\bf ascl:1102.012}
\footnote{http://arxiv.org/abs/astro-ph/0601706/}.

\subsection{Dendogram}

Also of note is the dendogram approach,



\subsection{duchamp / selavy}

Whiting (2011,2012) for 2D and 3D. Used in ASKAP pipeline. 
{\tt selavy} is the ASKAP 21-cm version. {\bf ascl:1201.011}
\footnote{http://adsabs.harvard.edu/abs/2012MNRAS.421.3242W}

\subsection{blobcat}

Hales et al. 2012.  2D.
{\bf ascl:1208.009}

\subsection{getsources}

Menshchikov et al. 2012. Herschell based. Two dimensional.
\footnote{http://adsabs.harvard.edu/abs/2012A\&A...542A..81M}
Code is not freely available yet, but is expected to be. Example
output made available to us.

\subsection{gaussclumps}

The original stutzki code is also available in GILDAS, as well as in
STARLINK.  It iteratively decomposes a 3D cube into a series of
Gaussian shaped clumps.  It starts by fitting a 3-dimensional clump
locally to the maximum of the input lbv cube. It then subtracts this
clump from the cube, creating a residual map, and then continues with
the maximum of this residual map.  The procedure is repeated until a
stop criterion is met, for instance when the maximum of the residual
maps drops below the 3 sigma level.

{\tt gaussclumps} was developed by Stutzki \& Guesten (1990, ApJ, 356, 513)
who applied it to their M17SW data sets. It was later described and
analyzed in more detail by Kramer et al. (1998, A\&A, 329, 249) who
also applied it to data cubes of various Galactic clouds.  The papers
of e.g. Heithausen et al. (1998) and Simon et al. (2000) give more examples
of the application of gaussclumps to large scale CO maps.

\subsection{imsad}

{\tt imsad} = Image Search and Destroy. 
Implemented in MIRIAD, based on {\tt imfit}, esssentially fits a gaussian
to a peak that sticks out of some plateau.

\subsection{SExtractor}

A widely source extractor (Bertin \& Arnouts 1996), but not so good for blended 
objects.
{\bf ascl:1010.064}

\subsection{SFind}

Hopkins et al. 2002.   Implemented in MIRIAD.

\subsection{MultiFind}
perl script using a series of miriad commands. Kilborn 2001. Written for HIPASS.

\subsection{tophat}

Meyer et al. 2004. For HIPASS. Uses a pattern matching of shapes and cross corre
lation
to find signals.

\subsection{lovedata}

Saintonge 2007. For ALFALFA. Written in IDL. Pattern matching.


\subsection{GammaFinder}

Boyce 2003. Masters thesis Cardiff.  In java. Use gamma function to estimate
noise and reject non-signal

\subsection{1d-1d wavelet}

C++ by FLoer and Winkel 2012.

\subsection{smooth plus clip}

An old method from the WSRT Bosma time, but more formally
introduced by Serra et al. 2011. ATLAS3D.

\subsection{CNHI}

Jurek 2012. for WALLABY project design study. CNHI = 
Characterised Noise HI Source Finder. Pattern matching. Also mentions the
Lutz (1980) one path algorithm, which is used to group regions from
the same source into a single source.


\subsection{DisPerSE}

DisPerSE is open source software for the identification of persistent
topological features such as peaks, voids, walls and in particular
filamentary structures within noisy sampled distributions in 2D,
3D. Sousbie (2013) {\bf ascl:1302.015}
\footnote{http://www2.iap.fr/users/sousbie/web/html/indexd41d.html}
 

Using DisPerSE, structure identification can be achieved through
the computation of the discrete Morse-Smale complex. The software can
deal directly with noisy datasets via the concept of persistence (a
measure of the robustness of topological features). Although developed
for the study of the properties of filamentary structures in the
cosmic web of galaxy distribution over large scales in the Universe,
the present version is quite versatile and should be useful for any
application where a robust structure identification is required, such
as for segmentation or for studying the topology of sampled functions
(for example, computing persistent Betti numbers). Currently, it can
be applied can work indifferently on many kinds of cell complex (such
as structured and unstructured grids, 2D manifolds embedded within a
3D space, discrete point samples using delaunay tesselation, and
Healpix tesselations of the sphere). The only constraint is that the
distribution must be defined over a manifold, possibly with
boundaries.

\subsection{FIVe}

Hacar et al. (2013) discuss a new Friends of Friends (FOF) 
algorithm they dubbed {\tt FIVe} (Friends in Velocity), and
implemented in the R scripting language.
\footnote{http://arxiv.org/abs/1303.2118}


\subsection{Kemper}

A multi-scale approach to automated feature extraction from HI
data cubes. 
So far only published as a research note, Kempker (2005) from RUG.


\subsection{PyFind}

% Paolo Serra <serra@astron.nl>
Serra (2012) at WSRT/ASTRON
has developed this not-yet-named code. PyFind is his current 
placeholder name. 

It is a simple Python code working on FITS files. It smooths
the cube using a set of specified kernels and finds emission at each
resolution by applying a specified threshold. The program applies also
a size filter at each resolution and makes a final mask as the union
of all masks. Two papers in 2012, one on MNRAS and one on PASA, discuss
it.

\subsection{GRID\_core}

Gong and Ostriker (2013),
coming from a more theoretical angle and 
instead of working with densities, developed 
{\tt GRID\_core} that works with
the gravitational potential.
In essense this is simply a more smooth version of
the density. It uses a watershed model (much like ClumpFind)
to define clumpy structures.  If the potential is not directly
available, a poisson solver can be be used to define densities
and with an assumed conversion factor, regions of bound structures
can be defined as well.  {\bf ascl:1302.007}


\subsection{TODO}

Here's some catch phrases that need to be tracked down if they have
relevant code for the source and feature extraction module.

GAIA, MATADOR, AstroMed, SPLAT-VO, Herschel DP, S2Plot, AstroMD. 


\section{Timeline and Deliverables}

In this section we describe the technical approach, the
timeline and the deliverables. Management Plan? 
All in a self-contained package.
2 years, 2.5 FTE is what we have on the whiteboard.

\subsection{Risk Factors}

\begin{enumerate}
\item
Pipeline Archive Integration (Low) : build on local fake archive?
\item
Pipeline Access API (Medium): user will manually have to download files
\item
Some tools too hard (Low): Deliver fewer tools

\end{enumerate}

\section { MARC: Overview }
The ALMA Data Mining Toolkit (ADMIT) is a software suite that operates on
ALMA data cubes. It creates descriptions that summarize to the user key
observational components of the data, as well as a number of "added value"
datasets to facilitate standard scientific analysis and to enable new and
creative ways to derive science from the cubes.  ADMIT is envisioned to
be run as part of the ALMA pipeline to create a new set of products for
the ALMA archive; however it is designed to also be capable of running
as a standalone application operating on user-downloaded cubes, including
recomputation (with perhaps modified parameters) of the products created
in the archive.

ADMIT consists of two main components. The core is the infrastructure layer
that defines data structures and programmatic pipeline; provides methods
for I/O, data selection, and computation of scientifically relevant
quantities; extracts and organizes scientific metadata from the cubes; and
defines the application programming interface (API) for higher-level tools.
Built upon the infrastructure layer are the ADMIT CASA Tools that provide
advanced functionality for scientific analysis.

\subsection{ Infrastructure Layer \label{s:infrastructure} }

% Using present tense here, but perhaps future tense is more appropriate.

ADMIT is fully defined and implemented in a CASA-compatible Python module.
The basic architecture is a pipeline with certain default image operations,
to which the user can add further operations.  The default pipeline
consists of the Summary, Moment, and Line Identification tools (see section
\ref{s:tools}).  ADMIT can work with images in both FITS and CASA format.
The outputs of the pipeline are wrapped into a single, self-describing
ZIP archive file, which any ADMIT CASA Tool can parse and manipulate.
A typical ZIP archive file might contain a descriptive XML file and 
associated small FITS and JPG files.

ADMIT is runnable from within the CASA Python environment, as individual
commands or a script.  However, for novice users we provide a simple
graphical user interface (GUI) for launching Tools and viewing results.

The core output of ADMIT is an XML file that contains all metadata
extracted from the science data, e.g.  project code, source name, sky
coordinates, ALMA bands used, list of spectral lines covered.  Each ADMIT
operation builds additional detail into the XML file creating a more
comprehensive, scientifically useful description.

In addition, ADMIT provides an infrastructure to operate on multiple
projects or sources, and perform the same operations on each of those. This
can be done either in parallel or recursively. It can then extract
information from each project and in essence mine a large suite of data,
allowing linked data techniques to visualize the extracted information
and provide new insight on what is common or different in the sources.
%% Are we providing a linked data viewer? Scope creep?


\subsection{ CASA Tools }

ADMIT defines a standard architecture that allows users to create and
"plug in" their own analysis tools to the ADMIT module.  ADMIT provides
an API for these tools to call existing CASA commands, manipulate images
or metadata, compute new quantities, return data structures to casapy,
and store results in the ZIP file with new metadata written to the XML file.

We describe here the N tools we will deliver at the end of the grant
period, as well as possible future tools.

{\it  PETER: for the moment I'm editing these in section \ref{s:tools}}

%a ZIP archive file that contains both the descriptive XML file, and all
%associated - but small - FITS and JPG files.


\section*{References}

\begin{enumerate}
\item
{\bf ASTUTE wiki:} {\tt  http://carma.astro.umd.edu/wiki/index.php/AStute}

\item
datasplot: URL

\item
Meier \& Turner, 2005. 2005ApJ...618..259M

\end{enumerate}

\end{document}

