\documentclass{article}
\title{Benchmarking CASA Image Cube Access}
\author{Peter Teuben}
%\date{\today}
\date{September, 2013}
\begin{document}
\maketitle

% \section*{Benchmark}

We conducted a modest benchmark to gain some insight in image
I/O patterns for a few packages easily accessible to us: CASA,
MIRIAD and NEMO. The first order goal is to understand if the
current CASA data access model is reasonably fast for general
access.
MIRIAD is the standard analysis package for
CARMA; NEMO is a stellar dynamics software tookbox which is
partially maintained by Peter Teuben. For these tests, we
used version 4.1.0 (r24668) of CASA. These test utilize 3-dimensional cubes
because they are the most common in ALMA data.

CASA uses a tiled storage manager, which in principle
can be tuned to optimize access speed for the specific
cube dimensions.  Currently, CASA does not actively tune
the tiling so these tests are done with CASA's standard tiling
(1/32 of an axis length).
MIRIAD has a traditional row based access; it rarely
uses memory to store full planes or cubes. NEMO is completely
memory based, and stores data either row or column based
(by default in double precision, but for this test NEMO was compiled 
in single precision).

Three datasets of 4GB ($10^9$ pixels)\footnote{4GB also happens to
be a good test to confirm there are no 32bit code issues left}
in size were used:
a 1024x1024x1024 true data ``cube'',
a 4096x4096x64 3-D ``slab'' and a 128x128x65536 3-D ``stick''.
Four operations were considered:  reading from fits
into the package native format,
a simple rms/mean statistics that visits the whole cube in the
most efficient way, adding two cubes, and finally
a Hanning smooth in X, Y and Z, where possible.  For MIRIAD
the Hanning smooth test had to be skipped in X and Y, unless
an additional {\tt reorder} step would be allowed, which we did
not consider in this version of the benchmark.
It should be added that a true Hanning in MIRIAD
is the slowest possible operation, since the data is stored
row-wise.

The tests were performed on a 3.6GHz i7-3820 CPU (10MB cache) with 64
GB of memory utilizing 1 core.  Raw disk I/O (using {\tt hdparm -t})
was measured at 800 MB/sec. Times reported are the wall clock time on
an idle system, to also observe the caching behavior of this pipeline.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l || r r r || r r r || r r r |}
\hline
        & \multicolumn{3}{|c|}  { Cube } 
        &  \multicolumn{3}{|c|} { Slab }
        &  \multicolumn{3}{|c|} { Stick } \\
        & \multicolumn{3}{|c|}  { 1024x1024x1024 }
        &  \multicolumn{3}{|c|} { 4096x4096x64 } 
        &  \multicolumn{3}{|c|} { 128x128x65536 } \\
        & CASA  & MIRIAD & NEMO    & CASA  & MIRIAD & NEMO  & CASA & MIRIAD & NEMO \\
%        & C     & M    & N         & C     & M     & N      & C    & M    & N \\
\hline
FITS    & 16.5 & 12.2  &  9.6/16.0  & 14.7  & 12.4   & 9.6/14.1  & 26.1  &  14.7  & 9.8/18.4 \\
STATS   & 17.1 & 14.0  &  9.8/10.4  & 17.7  & 13.8   & 7.2/9.4   & 17.9  &  15.2  & 7.3/10.1 \\
MATH    & 11.0 & 16.7  &  10.4/35.7 & 11.2  & 12.4   & 10.4/33.5 & 11.0  &  9.5   & 10.5/35.5  \\
HAN-x   & 15.6 &  n/a  &  12.3/8.9  & 12.6  & n/a    & 12.2/9.0  & 40.9  &  n/a   & 12.7/9.0 \\
HAN-y   & 16.8 &  n/a  &  16.5/17.3 & 29.7  & n/a    & 37.0/45.8 & 51.3  &  n/a   & 13.6/9.8 \\
HAN-z   & 24.8 & 112.0 & 61.3/106.5 & 72.0  & 102.5  & 24.1/44.7 & 18.4  &  137.0 & 40.8/46.2 \\
\hline 
sum-1     & 118.9 & 133.2 &   116.0/  & 157.9  &  138.2 & 100.0/   & 165.6 &  179.0 &  106.4/ \\
sum-2     & 109.3 & 124.0 &    95.3/  & 139.9  &  138.2 & 77.5/    & 142.5 &  187.3 &  74.0/ \\
\hline
\end{tabular}
\end{center}
\caption{Comparing I/O access in a ``cube'', ``slab'' and ``stick'' like dataset. 
Times reported
are the sum of user and system time, in seconds.
Columns designated are for C=CASA  M=MIRIAD N=NEMO(float).}
\end{table}


The tiled access in CASA performs well for true cubes, fairly
independent of the access direction, certainly much better than the
traditional methods of a memory based array.  But it does poorly in a
slab like cube, but outperforms anything else for a stick like
cube. The odd performance of the MATH case for NEMO is because of an
awkward interpreter used.

On a typical Linux workstation disk space is cached to memory. For
machines where there is at least several times of memory available
compared to the typical size of a datacube this can adversely affect
a pipelined benchmark such as the one given here by using the
disk cache in memory and thus report shorter processing times.
We therefore made sure
for each operation had no disk cache left\footnote{the command
{\tt "echo 1 $>$ /proc/sys/vm/drop\_caches"}, as root, was used for this}.


This also implies that running a pipeline on a dedicated machine
with plenty of memory is advantageous, as cached data is more likely
to be reused. The {\it sum-1} line in the table is essentially
a sum of the individual processes, as a worst case
scenario, {\it sum-2} a sum of 
these as a single pipeline step, potentially making use of caching
disk space in memory. As can be seen, in most cases a modest
gain is achieved.


\end{document}


